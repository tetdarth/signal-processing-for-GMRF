{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 実験概要\n",
    "## CMNによる特徴量抽出\n",
    "**ケプストラム平均正規化** (Cepstrum Mean Normalization : CMN) を使用し、特徴量を抽出する。\n",
    "切り出した波形に対して**離散フーリエ変換** (Discrete Fourier Transform : DFT) を行い、絶対値を取ることで**振幅スペクトル**を得る。\n",
    "これに対して対数を取って**対数振幅スペクトル**に変換し、逆離散フーリエ変換 (Inverse Discrete Fourier Transform : IDFT) を行うことで、ケプストラム領域へと変換し、先頭50要素を切り出し、結合した100要素の配列を前処理として返す。\n",
    "この前処理は\n",
    "```\n",
    "left, right, posture = pp.slicer(\"raw\\\\\" + tester.**.**.value)\n",
    "cepstrum = pp.cmn_denoise(left, right)\n",
    "```\n",
    "によって行われる。\n",
    "\n",
    "## DNNによる寝姿勢分類\n",
    "DNNによって前処理したデータから学習・分類を行う。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# モジュールのインポート"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "# 自作モジュール\n",
    "import datapath as dpath\n",
    "import preprocess as pp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataLoaderの定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from typing import Tuple\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "# 標準化 (平均 0 , 分散 1 )\n",
    "def standardization(a, axis=None, ddof=0):\n",
    "    a_mean = a.mean(axis=axis, keepdims=True)\n",
    "    a_std = a.std(axis=axis, keepdims=True, ddof=ddof)\n",
    "    return (a - a_mean) / a_std\n",
    "\n",
    "# 学習用データセットを読み込むためのDataLoaderを定義\n",
    "class train_datasets(Dataset):\n",
    "    def __init__(self, identity, transforms=None):\n",
    "        self.transform = transforms\n",
    "        self.type, self.tester, self.mattress = dpath.getattributes(identity)\n",
    "        print(self.type, self.tester, self.mattress)\n",
    "        print(\"train\")\n",
    "\n",
    "        # テスト以外のrawを読み込む\n",
    "        self.train_cepstrum = np.empty((0, 100))  # 100要素の配列\n",
    "        self.train_posture = np.empty(0)  # 姿勢データ配列\n",
    "\n",
    "        \n",
    "        # mattressに該当するrawのpathを読み込む\n",
    "        # train_paths = eval(f\"dpath.{self.type}.serch('{self.mattress}')\")\n",
    "        \n",
    "        train_paths = []\n",
    "        for mat in dpath.mattress_all():\n",
    "            if mat == self.mattress:\n",
    "                continue\n",
    "            train_paths.extend(eval(f\"dpath.{self.type}.serch('{mat}', skip=[dpath.{self.type}.{self.tester}])\"))\n",
    "        \n",
    "        for p in train_paths:\n",
    "            if identity.value == p:\n",
    "                continue\n",
    "            left, right, posture = pp.slicer(p)\n",
    "            cepstrum = pp.cmn_denoise(left, right)\n",
    "            for cep in cepstrum:\n",
    "                cep = standardization(cep)\n",
    "                self.train_cepstrum = np.vstack((self.train_cepstrum, cep)) if self.train_cepstrum.size else cep\n",
    "            self.train_posture = np.append(self.train_posture, posture) if self.train_posture.size else posture\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.train_posture)\n",
    "\n",
    "    def __getitem__(self, idx) -> Tuple[torch.tensor, torch.tensor]:\n",
    "        cepstrum = torch.tensor(self.train_cepstrum[idx].reshape(1, -1), dtype=torch.float32)\n",
    "        posture = torch.tensor(self.train_posture[idx]-1, dtype=torch.long)\n",
    "        if self.transform:\n",
    "            cepstrum = self.transform(cepstrum)\n",
    "        return cepstrum, posture\n",
    "\n",
    "\n",
    "# test用データセット\n",
    "class test_datasets(Dataset):\n",
    "    def __init__(self, identity, transform=None):\n",
    "        self.transform = transform\n",
    "        self.type, self.tester, self.mattress = dpath.getattributes(identity)\n",
    "        print(\"test\")\n",
    "\n",
    "        # 前処理したrawを読み込む\n",
    "        self.left, self.right, self.test_posture = pp.slicer(identity.value)\n",
    "        self.test_cepstrum = pp.cmn_denoise(self.left, self.right)\n",
    "        self.test_cepstrum = standardization(self.test_cepstrum)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.test_posture)\n",
    "\n",
    "    def __getitem__(self, idx) -> Tuple[torch.tensor, torch.tensor]:\n",
    "        cepstrum = torch.tensor(self.test_cepstrum[idx].reshape(1, -1), dtype=torch.float32)\n",
    "        posture = torch.tensor(self.test_posture[idx]-1, dtype=torch.long)\n",
    "        if self.transform:\n",
    "            cepstrum = self.transform(cepstrum)\n",
    "        return cepstrum, posture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 学習＆検証"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LMH M004 ka\n",
      "train\n",
      "raw\\LMH\\L003\\st_center | data[173 / 208]\n",
      "raw\\LMH\\H003\\st_center | data[340 / 403]\n",
      "raw\\LMH\\M003\\st_center | data[220 / 481]\n",
      "raw\\LMH\\M002\\st_center | data[174 / 438]\n",
      "raw\\LMH\\M001\\st_center | data[192 / 324]\n",
      "raw\\LMH\\H002\\st_center | data[194 / 230]\n",
      "raw\\LMH\\L001\\st_center | data[186 / 208]\n",
      "raw\\LMH\\H003\\fl_center | data[204 / 223]\n",
      "raw\\LMH\\M002\\flf_center | data[65 / 257]\n",
      "raw\\LMH\\M001\\fls_center | data[190 / 352]\n",
      "raw\\LMH\\M001\\flf_center | data[89 / 297]\n",
      "raw\\LMH\\M002\\fld_center | data[232 / 389]\n",
      "raw\\LMH\\M001\\fld_center | data[226 / 444]\n",
      "raw\\LMH\\M003\\fls_center | data[348 / 642]\n",
      "raw\\LMH\\M002\\fls_center | data[238 / 427]\n",
      "raw\\LMH\\L001\\fl_center | data[171 / 238]\n",
      "raw\\LMH\\M003\\flf_center | data[37 / 178]\n",
      "raw\\LMH\\H002\\fl_center | data[212 / 238]\n",
      "raw\\LMH\\M003\\fld_center | data[171 / 312]\n",
      "raw\\LMH\\L003\\fl_center | data[186 / 223]\n",
      "test\n",
      "raw\\LMH\\M004\\ka_center | data[125 / 306]\n"
     ]
    }
   ],
   "source": [
    "# 被験者\n",
    "identity = dpath.LMH.M004.value.ka_center\n",
    "\n",
    "train_val = train_datasets(identity)\n",
    "test = test_datasets(identity)\n",
    "batch_size = 64\n",
    "\n",
    "n_samples = len(train_val)\n",
    "train_size = int(n_samples * 0.8)    # [train : val] を [8 : 2] に分割\n",
    "val_size = n_samples - train_size\n",
    "\n",
    "train, val = torch.utils.data.random_split(train_val, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    drop_last=False\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    drop_last=False\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test,\n",
    "    shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss : 1.49082, acc : 0.28000\n",
      "Epoch[1/100] | train_loss: 0.72664 | train_acc: 0.71637 | val_loss: 0.41427 | val_acc: 0.83247\n",
      "loss : 1.48252, acc : 0.28000\n",
      "Epoch[2/100] | train_loss: 0.33606 | train_acc: 0.89311 | val_loss: 0.13949 | val_acc: 0.85325\n",
      "loss : 1.15346, acc : 0.20800\n",
      "Epoch[3/100] | train_loss: 0.32197 | train_acc: 0.88142 | val_loss: 0.10535 | val_acc: 0.90390\n",
      "loss : 1.08729, acc : 0.20800\n",
      "Epoch[4/100] | train_loss: 0.20919 | train_acc: 0.93015 | val_loss: 0.42253 | val_acc: 0.89870\n",
      "loss : 0.99444, acc : 0.20800\n",
      "Epoch[5/100] | train_loss: 0.18159 | train_acc: 0.93957 | val_loss: 0.37303 | val_acc: 0.94935\n",
      "loss : 1.40555, acc : 0.28000\n",
      "Epoch[6/100] | train_loss: 0.14601 | train_acc: 0.95419 | val_loss: 0.49517 | val_acc: 0.91429\n",
      "loss : 1.45772, acc : 0.52000\n",
      "Epoch[7/100] | train_loss: 0.18419 | train_acc: 0.93892 | val_loss: 0.37931 | val_acc: 0.94805\n",
      "loss : 1.13324, acc : 0.23200\n",
      "Epoch[8/100] | train_loss: 0.10632 | train_acc: 0.96394 | val_loss: 0.04206 | val_acc: 0.92857\n",
      "loss : 1.44108, acc : 0.41600\n",
      "Epoch[9/100] | train_loss: 0.18032 | train_acc: 0.93795 | val_loss: 0.04982 | val_acc: 0.94545\n",
      "loss : 1.40313, acc : 0.48000\n",
      "Epoch[10/100] | train_loss: 0.11622 | train_acc: 0.96264 | val_loss: 0.09900 | val_acc: 0.93377\n",
      "loss : 1.24843, acc : 0.28000\n",
      "Epoch[11/100] | train_loss: 0.14953 | train_acc: 0.94769 | val_loss: 0.14826 | val_acc: 0.94675\n",
      "loss : 1.12273, acc : 0.20800\n",
      "Epoch[12/100] | train_loss: 0.06904 | train_acc: 0.97888 | val_loss: 0.00794 | val_acc: 0.96753\n",
      "loss : 1.50080, acc : 0.28000\n",
      "Epoch[13/100] | train_loss: 0.07286 | train_acc: 0.98376 | val_loss: 0.00517 | val_acc: 0.96623\n",
      "loss : 1.77458, acc : 0.34400\n",
      "Epoch[14/100] | train_loss: 0.15388 | train_acc: 0.94704 | val_loss: 0.00276 | val_acc: 0.93636\n",
      "loss : 1.57436, acc : 0.28000\n",
      "Epoch[15/100] | train_loss: 0.07901 | train_acc: 0.97986 | val_loss: 0.01632 | val_acc: 0.95844\n",
      "loss : 1.51678, acc : 0.28000\n",
      "Epoch[16/100] | train_loss: 0.09699 | train_acc: 0.96686 | val_loss: 0.25964 | val_acc: 0.95844\n",
      "loss : 1.46804, acc : 0.28000\n",
      "Epoch[17/100] | train_loss: 0.07477 | train_acc: 0.98148 | val_loss: 0.13060 | val_acc: 0.94805\n",
      "loss : 1.55964, acc : 0.28000\n",
      "Epoch[18/100] | train_loss: 0.13714 | train_acc: 0.95354 | val_loss: 0.00377 | val_acc: 0.95714\n",
      "loss : 1.76505, acc : 0.28000\n",
      "Epoch[19/100] | train_loss: 0.06909 | train_acc: 0.97661 | val_loss: 0.70139 | val_acc: 0.97403\n",
      "loss : 1.47620, acc : 0.28000\n",
      "Epoch[20/100] | train_loss: 0.10883 | train_acc: 0.96361 | val_loss: 0.07511 | val_acc: 0.91818\n",
      "loss : 1.72611, acc : 0.28000\n",
      "Epoch[21/100] | train_loss: 0.13956 | train_acc: 0.96524 | val_loss: 0.01160 | val_acc: 0.96883\n",
      "loss : 1.52339, acc : 0.28000\n",
      "Epoch[22/100] | train_loss: 0.13486 | train_acc: 0.95744 | val_loss: 0.02345 | val_acc: 0.97662\n",
      "loss : 1.99705, acc : 0.28000\n",
      "Epoch[23/100] | train_loss: 0.07748 | train_acc: 0.97336 | val_loss: 0.37737 | val_acc: 0.96364\n",
      "loss : 1.31092, acc : 0.28000\n",
      "Epoch[24/100] | train_loss: 0.08798 | train_acc: 0.99188 | val_loss: 0.01651 | val_acc: 0.95455\n",
      "loss : 1.67515, acc : 0.28000\n",
      "Epoch[25/100] | train_loss: 0.20197 | train_acc: 0.93372 | val_loss: 0.69488 | val_acc: 0.95325\n",
      "loss : 1.55606, acc : 0.28000\n",
      "Epoch[26/100] | train_loss: 0.09009 | train_acc: 0.97238 | val_loss: 0.22184 | val_acc: 0.97143\n",
      "loss : 1.43037, acc : 0.28800\n",
      "Epoch[27/100] | train_loss: 0.06773 | train_acc: 0.98506 | val_loss: 0.55729 | val_acc: 0.97013\n",
      "loss : 1.02372, acc : 0.38400\n",
      "Epoch[28/100] | train_loss: 0.10561 | train_acc: 0.96979 | val_loss: 0.03691 | val_acc: 0.95455\n",
      "loss : 0.96592, acc : 0.44000\n",
      "Epoch[29/100] | train_loss: 0.03439 | train_acc: 0.99155 | val_loss: 0.07756 | val_acc: 0.96623\n",
      "loss : 1.24801, acc : 0.28000\n",
      "Epoch[30/100] | train_loss: 0.06838 | train_acc: 0.97823 | val_loss: 0.22861 | val_acc: 0.96883\n",
      "loss : 0.96533, acc : 0.36000\n",
      "Epoch[31/100] | train_loss: 0.05834 | train_acc: 0.97921 | val_loss: 0.04890 | val_acc: 0.97143\n",
      "loss : 0.97367, acc : 0.43200\n",
      "Epoch[32/100] | train_loss: 0.03604 | train_acc: 0.99123 | val_loss: 0.87702 | val_acc: 0.97013\n",
      "loss : 0.82461, acc : 0.43200\n",
      "Epoch[33/100] | train_loss: 0.06595 | train_acc: 0.97953 | val_loss: 0.53780 | val_acc: 0.97143\n",
      "loss : 1.07025, acc : 0.33600\n",
      "Epoch[34/100] | train_loss: 0.01596 | train_acc: 0.99643 | val_loss: 0.00318 | val_acc: 0.98182\n",
      "loss : 0.87555, acc : 0.40800\n",
      "Epoch[35/100] | train_loss: 0.01854 | train_acc: 0.99513 | val_loss: 0.20488 | val_acc: 0.97403\n",
      "loss : 0.88513, acc : 0.34400\n",
      "Epoch[36/100] | train_loss: 0.01884 | train_acc: 0.99513 | val_loss: 0.13941 | val_acc: 0.97792\n",
      "loss : 1.51395, acc : 0.28000\n",
      "Epoch[37/100] | train_loss: 0.02230 | train_acc: 0.99253 | val_loss: 0.02053 | val_acc: 0.96623\n",
      "loss : 1.40315, acc : 0.28000\n",
      "Epoch[38/100] | train_loss: 0.00915 | train_acc: 0.99675 | val_loss: 0.03679 | val_acc: 0.98442\n",
      "loss : 1.26579, acc : 0.28000\n",
      "Epoch[39/100] | train_loss: 0.00517 | train_acc: 0.99903 | val_loss: 0.00127 | val_acc: 0.97922\n",
      "loss : 1.13351, acc : 0.28000\n",
      "Epoch[40/100] | train_loss: 0.00907 | train_acc: 0.99968 | val_loss: 0.38019 | val_acc: 0.97532\n",
      "loss : 1.21712, acc : 0.28000\n",
      "Epoch[41/100] | train_loss: 0.07071 | train_acc: 0.97596 | val_loss: 0.31670 | val_acc: 0.96753\n",
      "loss : 1.18833, acc : 0.28000\n",
      "Epoch[42/100] | train_loss: 0.02604 | train_acc: 0.99740 | val_loss: 0.00794 | val_acc: 0.97403\n",
      "loss : 1.63881, acc : 0.39200\n",
      "Epoch[43/100] | train_loss: 0.14605 | train_acc: 0.95484 | val_loss: 0.35619 | val_acc: 0.94805\n",
      "loss : 1.35965, acc : 0.28800\n",
      "Epoch[44/100] | train_loss: 0.07454 | train_acc: 0.98798 | val_loss: 0.00151 | val_acc: 0.97922\n",
      "loss : 2.11899, acc : 0.28000\n",
      "Epoch[45/100] | train_loss: 0.10732 | train_acc: 0.96979 | val_loss: 0.01829 | val_acc: 0.96234\n",
      "loss : 2.25428, acc : 0.37600\n",
      "Epoch[46/100] | train_loss: 0.08341 | train_acc: 0.97206 | val_loss: 0.32948 | val_acc: 0.97532\n",
      "loss : 2.13882, acc : 0.28000\n",
      "Epoch[47/100] | train_loss: 0.01831 | train_acc: 0.99578 | val_loss: 0.00112 | val_acc: 0.97662\n",
      "loss : 1.89331, acc : 0.28000\n",
      "Epoch[48/100] | train_loss: 0.01583 | train_acc: 0.99578 | val_loss: 0.64690 | val_acc: 0.97403\n",
      "loss : 2.03359, acc : 0.28000\n",
      "Epoch[49/100] | train_loss: 0.01108 | train_acc: 0.99805 | val_loss: 0.00549 | val_acc: 0.97922\n",
      "loss : 1.59795, acc : 0.28000\n",
      "Epoch[50/100] | train_loss: 0.00789 | train_acc: 0.99870 | val_loss: 0.65355 | val_acc: 0.97532\n",
      "loss : 2.12619, acc : 0.28000\n",
      "Epoch[51/100] | train_loss: 0.00677 | train_acc: 0.99838 | val_loss: 0.03000 | val_acc: 0.97922\n",
      "loss : 1.68132, acc : 0.28000\n",
      "Epoch[52/100] | train_loss: 0.01379 | train_acc: 0.99643 | val_loss: 0.00123 | val_acc: 0.96883\n",
      "loss : 1.74920, acc : 0.28000\n",
      "Epoch[53/100] | train_loss: 0.00837 | train_acc: 0.99708 | val_loss: 0.00040 | val_acc: 0.97143\n",
      "loss : 1.72519, acc : 0.28000\n",
      "Epoch[54/100] | train_loss: 0.01176 | train_acc: 0.99740 | val_loss: 0.00351 | val_acc: 0.97273\n",
      "loss : 1.88835, acc : 0.28000\n",
      "Epoch[55/100] | train_loss: 0.01720 | train_acc: 0.99545 | val_loss: 0.00817 | val_acc: 0.97013\n",
      "loss : 2.04047, acc : 0.28000\n",
      "Epoch[56/100] | train_loss: 0.02786 | train_acc: 0.99188 | val_loss: 0.00230 | val_acc: 0.97532\n",
      "loss : 1.95098, acc : 0.28000\n",
      "Epoch[57/100] | train_loss: 0.01525 | train_acc: 0.99643 | val_loss: 0.07862 | val_acc: 0.97792\n",
      "loss : 1.73354, acc : 0.28000\n",
      "Epoch[58/100] | train_loss: 0.00235 | train_acc: 1.00000 | val_loss: 1.00314 | val_acc: 0.98442\n",
      "loss : 1.68709, acc : 0.28000\n",
      "Epoch[59/100] | train_loss: 0.00337 | train_acc: 1.00000 | val_loss: 0.00276 | val_acc: 0.98571\n",
      "loss : 1.94275, acc : 0.28000\n",
      "Epoch[60/100] | train_loss: 0.02322 | train_acc: 0.99383 | val_loss: 0.00076 | val_acc: 0.97273\n",
      "loss : 1.20022, acc : 0.39200\n",
      "Epoch[61/100] | train_loss: 0.01592 | train_acc: 0.99480 | val_loss: 0.35465 | val_acc: 0.98052\n",
      "loss : 1.20225, acc : 0.36000\n",
      "Epoch[62/100] | train_loss: 0.02384 | train_acc: 0.99773 | val_loss: 0.01657 | val_acc: 0.98571\n",
      "loss : 1.12945, acc : 0.30400\n",
      "Epoch[63/100] | train_loss: 0.04571 | train_acc: 0.98571 | val_loss: 0.25171 | val_acc: 0.98052\n",
      "loss : 1.47213, acc : 0.28000\n",
      "Epoch[64/100] | train_loss: 0.01796 | train_acc: 0.99350 | val_loss: 0.01015 | val_acc: 0.98571\n",
      "loss : 1.42984, acc : 0.29600\n",
      "Epoch[65/100] | train_loss: 0.01523 | train_acc: 0.99870 | val_loss: 0.00187 | val_acc: 0.97532\n",
      "loss : 0.61325, acc : 0.28800\n",
      "Epoch[66/100] | train_loss: 0.05435 | train_acc: 0.99025 | val_loss: 0.06523 | val_acc: 0.97143\n",
      "loss : 0.78705, acc : 0.36000\n",
      "Epoch[67/100] | train_loss: 0.05341 | train_acc: 0.98408 | val_loss: 0.00429 | val_acc: 0.97273\n",
      "loss : 0.50056, acc : 0.24000\n",
      "Epoch[68/100] | train_loss: 0.03034 | train_acc: 0.99610 | val_loss: 0.08031 | val_acc: 0.97273\n",
      "loss : 0.54739, acc : 0.39200\n",
      "Epoch[69/100] | train_loss: 0.03186 | train_acc: 0.98765 | val_loss: 0.00095 | val_acc: 0.97662\n",
      "loss : 0.59042, acc : 0.37600\n",
      "Epoch[70/100] | train_loss: 0.00486 | train_acc: 1.00000 | val_loss: 0.00132 | val_acc: 0.98052\n",
      "loss : 0.68395, acc : 0.36800\n",
      "Epoch[71/100] | train_loss: 0.00470 | train_acc: 0.99968 | val_loss: 0.04781 | val_acc: 0.98052\n",
      "loss : 0.64619, acc : 0.35200\n",
      "Epoch[72/100] | train_loss: 0.00396 | train_acc: 0.99968 | val_loss: 0.00656 | val_acc: 0.98831\n",
      "loss : 0.87261, acc : 0.40800\n",
      "Epoch[73/100] | train_loss: 0.01324 | train_acc: 0.99903 | val_loss: 0.67856 | val_acc: 0.98442\n",
      "loss : 1.01923, acc : 0.42400\n",
      "Epoch[74/100] | train_loss: 0.03529 | train_acc: 0.99058 | val_loss: 0.02188 | val_acc: 0.97143\n",
      "loss : 0.96123, acc : 0.42400\n",
      "Epoch[75/100] | train_loss: 0.00470 | train_acc: 0.99903 | val_loss: 0.64584 | val_acc: 0.97403\n",
      "loss : 1.06503, acc : 0.42400\n",
      "Epoch[76/100] | train_loss: 0.00264 | train_acc: 1.00000 | val_loss: 0.06203 | val_acc: 0.97792\n",
      "loss : 0.98527, acc : 0.43200\n",
      "Epoch[77/100] | train_loss: 0.00266 | train_acc: 1.00000 | val_loss: 0.43229 | val_acc: 0.98052\n",
      "loss : 0.89531, acc : 0.43200\n",
      "Epoch[78/100] | train_loss: 0.00465 | train_acc: 0.99870 | val_loss: 0.72844 | val_acc: 0.98182\n",
      "loss : 0.96186, acc : 0.42400\n",
      "Epoch[79/100] | train_loss: 0.00147 | train_acc: 1.00000 | val_loss: 0.00008 | val_acc: 0.98182\n",
      "loss : 0.96710, acc : 0.43200\n",
      "Epoch[80/100] | train_loss: 0.00215 | train_acc: 0.99968 | val_loss: 0.27361 | val_acc: 0.97662\n",
      "loss : 1.04060, acc : 0.42400\n",
      "Epoch[81/100] | train_loss: 0.00132 | train_acc: 1.00000 | val_loss: 0.01555 | val_acc: 0.97922\n",
      "loss : 1.05789, acc : 0.42400\n",
      "Epoch[82/100] | train_loss: 0.00070 | train_acc: 1.00000 | val_loss: 0.61875 | val_acc: 0.98182\n",
      "loss : 0.97821, acc : 0.43200\n",
      "Epoch[83/100] | train_loss: 0.00135 | train_acc: 0.99968 | val_loss: 0.00060 | val_acc: 0.98182\n",
      "loss : 1.06624, acc : 0.39200\n",
      "Epoch[84/100] | train_loss: 0.00562 | train_acc: 0.99968 | val_loss: 0.63516 | val_acc: 0.97922\n",
      "loss : 1.34222, acc : 0.31200\n",
      "Epoch[85/100] | train_loss: 0.02302 | train_acc: 0.99545 | val_loss: 0.11089 | val_acc: 0.97273\n",
      "loss : 1.92485, acc : 0.28000\n",
      "Epoch[86/100] | train_loss: 0.06948 | train_acc: 0.97563 | val_loss: 0.00337 | val_acc: 0.97922\n",
      "loss : 1.50606, acc : 0.28000\n",
      "Epoch[87/100] | train_loss: 0.01075 | train_acc: 0.99740 | val_loss: 0.00174 | val_acc: 0.97273\n",
      "loss : 1.47198, acc : 0.28800\n",
      "Epoch[88/100] | train_loss: 0.00501 | train_acc: 0.99903 | val_loss: 0.00234 | val_acc: 0.98831\n",
      "loss : 1.41233, acc : 0.30400\n",
      "Epoch[89/100] | train_loss: 0.01064 | train_acc: 0.99740 | val_loss: 0.39233 | val_acc: 0.97922\n",
      "loss : 1.56191, acc : 0.28000\n",
      "Epoch[90/100] | train_loss: 0.01923 | train_acc: 0.99448 | val_loss: 0.41618 | val_acc: 0.98442\n",
      "loss : 1.56610, acc : 0.28000\n",
      "Epoch[91/100] | train_loss: 0.00535 | train_acc: 0.99935 | val_loss: 0.01707 | val_acc: 0.98701\n",
      "loss : 1.42763, acc : 0.28800\n",
      "Epoch[92/100] | train_loss: 0.00684 | train_acc: 0.99838 | val_loss: 0.00437 | val_acc: 0.98052\n",
      "loss : 1.59007, acc : 0.28000\n",
      "Epoch[93/100] | train_loss: 0.00319 | train_acc: 0.99935 | val_loss: 0.51721 | val_acc: 0.97532\n",
      "loss : 1.50267, acc : 0.28000\n",
      "Epoch[94/100] | train_loss: 0.00161 | train_acc: 1.00000 | val_loss: 0.00072 | val_acc: 0.98442\n",
      "loss : 1.41466, acc : 0.28000\n",
      "Epoch[95/100] | train_loss: 0.00133 | train_acc: 1.00000 | val_loss: 0.00034 | val_acc: 0.98571\n",
      "loss : 1.46697, acc : 0.28800\n",
      "Epoch[96/100] | train_loss: 0.00564 | train_acc: 0.99968 | val_loss: 0.42713 | val_acc: 0.98442\n",
      "loss : 1.75327, acc : 0.28000\n",
      "Epoch[97/100] | train_loss: 0.03833 | train_acc: 0.98993 | val_loss: 0.39334 | val_acc: 0.97273\n",
      "loss : 1.48210, acc : 0.28000\n",
      "Epoch[98/100] | train_loss: 0.01039 | train_acc: 0.99935 | val_loss: 0.13027 | val_acc: 0.97273\n",
      "loss : 1.01608, acc : 0.28800\n",
      "Epoch[99/100] | train_loss: 0.08397 | train_acc: 0.97986 | val_loss: 0.06550 | val_acc: 0.96883\n",
      "loss : 1.15384, acc : 0.28000\n",
      "Epoch[100/100] | train_loss: 0.00618 | train_acc: 0.99805 | val_loss: 0.11963 | val_acc: 0.97922\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import model_resnet as resnet\n",
    "import model_har as har\n",
    "\n",
    "# モデルのインスタンス化\n",
    "net = har.HAR(num_classes=4).to(device)\n",
    "\n",
    "# 誤差関数を交差エントロピーで計算\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# 最適化アルゴリズム\n",
    "lr = 8e-4\n",
    "optimizer = optim.Adam(net.parameters(), lr = lr, weight_decay=1e-4)\n",
    "\n",
    "# 学習\n",
    "n_epoch = 100\n",
    "for epoch in range(n_epoch):\n",
    "    if (epoch+1)%50 == 0:\n",
    "        lr *= 0.4\n",
    "        optimizer = optim.Adam(net.parameters(), lr = lr, weight_decay=1e-4)\n",
    "    \n",
    "    # 精度と損失の初期化\n",
    "    train_acc, train_loss = 0, 0\n",
    "    val_acc, val_loss = 0, 0\n",
    "    n_train, n_val = 0, 0\n",
    "    test_loss, test_acc = 0, 0\n",
    "    n_test = 0\n",
    "\n",
    "    # 学習\n",
    "    for train_input, train_label in train_loader:\n",
    "        n_train += len(train_label)\n",
    "\n",
    "        # 入力と正解ラベルをGPU上に移動\n",
    "        input = train_input.to(device)\n",
    "        label = train_label.to(device)\n",
    "        # print(f'input : {input.shape}, label : {label.shape}')\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = net(input)\n",
    "        loss = criterion(output, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        predicted = torch.max(output, 1)[1]\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        train_acc += (predicted == label).sum().item()\n",
    "\n",
    "    # 検証\n",
    "    for val_input, val_label in val_loader:\n",
    "        n_val += len(val_label)\n",
    "\n",
    "        val_input = val_input.to(device)\n",
    "        val_label = val_label.to(device)\n",
    "\n",
    "        val_output = net(val_input)\n",
    "        val_loss = criterion(val_output, val_label)\n",
    "\n",
    "        val_predicted = torch.max(val_output, 1)[1]\n",
    "\n",
    "        val_loss += val_loss.item()\n",
    "        val_acc += (val_predicted == val_label).sum().item()\n",
    "\n",
    "    # テスト\n",
    "    for test_input, test_label in test_loader:\n",
    "        n_test += len(test_label)\n",
    "\n",
    "        test_input = test_input.to(device)\n",
    "        test_label = test_label.to(device)\n",
    "\n",
    "        test_output = net(test_input)\n",
    "        test_loss = criterion(test_output, test_label)\n",
    "\n",
    "        test_predicted = torch.max(test_output, 1)[1]\n",
    "\n",
    "        test_loss += test_loss.item()\n",
    "        test_acc += (test_predicted == test_label).sum().item()\n",
    "        \n",
    "    # 精度を確率に変換\n",
    "    test_acc /= n_test\n",
    "    test_loss = test_loss * batch_size / n_test\n",
    "\n",
    "    print(f\"loss : {test_loss:.5f}, acc : {test_acc:.5f}\")\n",
    "\n",
    "    # 精度を確率に変換\n",
    "    train_acc /= n_train\n",
    "    val_acc /= n_val\n",
    "    train_loss = train_loss * batch_size / n_train\n",
    "    val_loss = val_loss * batch_size / n_val\n",
    "\n",
    "    if not epoch%1:\n",
    "        print(f\"Epoch[{epoch+1}/{n_epoch}] | train_loss: {train_loss:.5f} | train_acc: {train_acc:.5f} | val_loss: {val_loss:.5f} | val_acc: {val_acc:.5f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 推定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss : 1.15384, acc : 0.28000\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = 0, 0\n",
    "n_test = 0\n",
    "\n",
    "for test_input, test_label in test_loader:\n",
    "        n_test += len(test_label)\n",
    "\n",
    "        test_input = test_input.to(device)\n",
    "        test_label = test_label.to(device)\n",
    "\n",
    "        test_output = net(test_input)\n",
    "        test_loss = criterion(test_output, test_label)\n",
    "\n",
    "        test_predicted = torch.max(test_output, 1)[1]\n",
    "\n",
    "        test_loss += test_loss.item()\n",
    "        test_acc += (test_predicted == test_label).sum().item()\n",
    "        \n",
    "# 精度を確率に変換\n",
    "test_acc /= n_test\n",
    "test_loss = test_loss * batch_size / n_test\n",
    "\n",
    "print(f\"loss : {test_loss:.5f}, acc : {test_acc:.5f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
