{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 実験概要\n",
    "## CMNによる特徴量抽出\n",
    "**ケプストラム平均正規化** (Cepstrum Mean Normalization : CMN) を使用し、特徴量を抽出する。\n",
    "切り出した波形に対して**離散フーリエ変換** (Discrete Fourier Transform : DFT) を行い、絶対値を取ることで**振幅スペクトル**を得る。\n",
    "これに対して対数を取って**対数振幅スペクトル**に変換し、逆離散フーリエ変換 (Inverse Discrete Fourier Transform : IDFT) を行うことで、ケプストラム領域へと変換し、先頭50要素を切り出し、結合した100要素の配列を前処理として返す。\n",
    "この前処理は\n",
    "```\n",
    "left, right, posture = pp.slicer(\"raw\\\\\" + tester.**.**.value)\n",
    "cepstrum = pp.cmn_denoise(left, right)\n",
    "```\n",
    "によって行われる。\n",
    "\n",
    "## DNNによる寝姿勢分類\n",
    "DNNによって前処理したデータから学習・分類を行う。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# モジュールのインポート"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "# 自作モジュール\n",
    "import datapath as dpath\n",
    "import preprocess as pp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataLoaderの定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tester以外のrawを読み込む\n",
    "for p in glob.glob(\".\\\\raw\\\\LMH\\\\*\\\\\", recursive=True):\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw\\LMH\\H002\\fl_center\n",
      "type : LMH,  tester : H002,  mattress : fl\n",
      "[WindowsPath('raw/LMH/H002/fl_center'), WindowsPath('raw/LMH/H003/fl_center'), WindowsPath('raw/LMH/L001/fl_center'), WindowsPath('raw/LMH/L003/fl_center'), WindowsPath('raw/LMH/M001/fls_center'), WindowsPath('raw/LMH/M001/fld_center'), WindowsPath('raw/LMH/M001/fls_center'), WindowsPath('raw/LMH/M002/fls_center'), WindowsPath('raw/LMH/M002/fld_center'), WindowsPath('raw/LMH/M002/fls_center'), WindowsPath('raw/LMH/M003/fls_center'), WindowsPath('raw/LMH/M003/fld_center'), WindowsPath('raw/LMH/M003/fls_center'), WindowsPath('raw/LMH/M004/fls_center'), WindowsPath('raw/LMH/M004/fld_center'), WindowsPath('raw/LMH/M004/fls_center'), WindowsPath('raw/LMH/H002/fl_center'), WindowsPath('raw/LMH/H003/fl_center'), WindowsPath('raw/LMH/L001/fl_center'), WindowsPath('raw/LMH/L003/fl_center'), WindowsPath('raw/LMH/M001/fls_center'), WindowsPath('raw/LMH/M001/fld_center'), WindowsPath('raw/LMH/M001/fls_center'), WindowsPath('raw/LMH/M002/fls_center'), WindowsPath('raw/LMH/M002/fld_center'), WindowsPath('raw/LMH/M002/fls_center'), WindowsPath('raw/LMH/M003/fls_center'), WindowsPath('raw/LMH/M003/fld_center'), WindowsPath('raw/LMH/M003/fls_center'), WindowsPath('raw/LMH/M004/fls_center'), WindowsPath('raw/LMH/M004/fld_center'), WindowsPath('raw/LMH/M004/fls_center')]\n"
     ]
    }
   ],
   "source": [
    "e = dpath.LMH.H002.value.fl_center\n",
    "print(e.value)\n",
    "\n",
    "type, tester, mattress = dpath.getattributes(e, position=False)\n",
    "print(f\"type : {type},  tester : {tester},  mattress : {mattress}\")\n",
    "train_paths = eval(f\"dpath.{type}.serch('{mattress}')\")\n",
    "print(train_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from typing import Tuple\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "# 学習用データセットを読み込むためのDataLoaderを定義\n",
    "class train_datasets(Dataset):\n",
    "    def __init__(self, identity, transforms=None):\n",
    "        self.transform = transforms\n",
    "        self.type, self.tester, self.mattress = dpath.getattributes(identity)\n",
    "        print(self.type, self.tester, self.mattress)\n",
    "        print(\"train\")\n",
    "\n",
    "        # テスト以外のrawを読み込む\n",
    "        self.train_cepstrum = np.empty((0, 100))  # 100要素の配列\n",
    "        self.train_posture = np.empty(0)  # 姿勢データ配列\n",
    "\n",
    "        # mattressに該当するrawのpathを読み込む\n",
    "        train_paths = eval(f\"dpath.{self.type}.serch('{self.mattress}')\")\n",
    "        for p in train_paths:\n",
    "            if identity.value == p:\n",
    "                continue\n",
    "            left, right, posture = pp.slicer(p)\n",
    "            cepstrum = pp.cmn_denoise(left, right)\n",
    "            for c in cepstrum:\n",
    "                self.train_cepstrum = np.vstack((self.train_cepstrum, c)) if self.train_cepstrum.size else c\n",
    "            self.train_posture = np.append(self.train_posture, posture) if self.train_posture.size else posture\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.train_posture)\n",
    "\n",
    "    def __getitem__(self, idx) -> Tuple[torch.tensor, torch.tensor]:\n",
    "        cepstrum = torch.tensor(self.train_cepstrum[idx].reshape(1, -1), dtype=torch.float32)\n",
    "        posture = torch.tensor(self.train_posture[idx]-1, dtype=torch.long)\n",
    "        if self.transform:\n",
    "            cepstrum = self.transform(cepstrum)\n",
    "        return cepstrum, posture\n",
    "\n",
    "\n",
    "# test用データセット\n",
    "class test_datasets(Dataset):\n",
    "    def __init__(self, identity, transform=None):\n",
    "        self.transform = transform\n",
    "        self.type, self.tester, self.mattress = dpath.getattributes(identity)\n",
    "        print(\"test\")\n",
    "\n",
    "        # 前処理したrawを読み込む\n",
    "        self.left, self.right, self.test_posture = pp.slicer(identity.value)\n",
    "        self.test_cepstrum = pp.cmn_denoise(self.left, self.right)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.test_posture)\n",
    "\n",
    "    def __getitem__(self, idx) -> Tuple[torch.tensor, torch.tensor]:\n",
    "        cepstrum = torch.tensor(self.test_cepstrum[idx].reshape(1, -1), dtype=torch.float32)\n",
    "        posture = torch.tensor(self.test_posture[idx]-1, dtype=torch.long)\n",
    "        if self.transform:\n",
    "            cepstrum = self.transform(cepstrum)\n",
    "        return cepstrum, posture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 学習＆検証"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデル定義\n",
    "1次元ResNetを定義する。nn.Conv1dを用いて残差ブロックを定義する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# ボトルネック残差ブロックの定義（1D用）\n",
    "class Bottleneck1D(nn.Module):\n",
    "    expansion = 4  # ボトルネックの出力チャンネルは、最初の畳み込み層の出力チャンネルの4倍\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n",
    "        super(Bottleneck1D, self).__init__()\n",
    "        # 1x1 畳み込み層（チャンネル数の縮小）\n",
    "        self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size=1, stride=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm1d(out_channels)\n",
    "        # 3x3 畳み込み層\n",
    "        self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm1d(out_channels)\n",
    "        # 1x1 畳み込み層（チャンネル数の拡張）\n",
    "        self.conv3 = nn.Conv1d(out_channels, out_channels * self.expansion, kernel_size=1, stride=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm1d(out_channels * self.expansion)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "# ResNetの定義（1D用、ResNet-50）\n",
    "class ResNet1D(nn.Module):\n",
    "    def __init__(self, block, layers, num_classes=10):\n",
    "        super(ResNet1D, self).__init__()\n",
    "        self.in_channels = 64\n",
    "        self.conv1 = nn.Conv1d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool1d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, out_channels, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.in_channels != out_channels * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv1d(self.in_channels, out_channels * block.expansion, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm1d(out_channels * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.in_channels, out_channels, stride, downsample))\n",
    "        self.in_channels = out_channels * block.expansion\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(self.in_channels, out_channels))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "# ResNet-50の構造を定義\n",
    "def resnet1d50(num_classes=4):\n",
    "    return ResNet1D(Bottleneck1D, [3, 4, 6, 3], num_classes=num_classes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ResNetを定義する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LMH H002 st\n",
      "train\n",
      "data[340 / 403]\n",
      "data[186 / 208]\n",
      "data[173 / 208]\n",
      "data[192 / 324]\n",
      "data[174 / 438]\n",
      "data[220 / 481]\n",
      "data[87 / 281]\n",
      "data[340 / 403]\n",
      "data[186 / 208]\n",
      "data[173 / 208]\n",
      "data[192 / 324]\n",
      "data[174 / 438]\n",
      "data[220 / 481]\n",
      "data[87 / 281]\n",
      "test\n",
      "data[194 / 230]\n"
     ]
    }
   ],
   "source": [
    "identity = dpath.LMH.H002.value.st_center\n",
    "\n",
    "train = train_datasets(identity)\n",
    "test = test_datasets(identity)\n",
    "batch_size = 128\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    drop_last=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[1/500], | loss: 1.81214 | acc: 0.28609 | val_loss: 1.82350 | val_acc: 0.27604\n",
      "Epoch[2/500], | loss: 1.59358 | acc: 0.28906 | val_loss: 1.91654 | val_acc: 0.23958\n",
      "Epoch[3/500], | loss: 1.54906 | acc: 0.28497 | val_loss: 1.81449 | val_acc: 0.22396\n",
      "Epoch[4/500], | loss: 1.48984 | acc: 0.29315 | val_loss: 1.81823 | val_acc: 0.24479\n",
      "Epoch[5/500], | loss: 1.49047 | acc: 0.28088 | val_loss: 1.85839 | val_acc: 0.20833\n",
      "Epoch[6/500], | loss: 1.45752 | acc: 0.29874 | val_loss: 1.75058 | val_acc: 0.21354\n",
      "Epoch[7/500], | loss: 1.44125 | acc: 0.30618 | val_loss: 1.66974 | val_acc: 0.22396\n",
      "Epoch[8/500], | loss: 1.44245 | acc: 0.30915 | val_loss: 1.62230 | val_acc: 0.27604\n",
      "Epoch[9/500], | loss: 1.44419 | acc: 0.29427 | val_loss: 1.74762 | val_acc: 0.20312\n",
      "Epoch[10/500], | loss: 1.44011 | acc: 0.31250 | val_loss: 1.67760 | val_acc: 0.26562\n",
      "Epoch[11/500], | loss: 1.43255 | acc: 0.31101 | val_loss: 1.72955 | val_acc: 0.20833\n",
      "Epoch[12/500], | loss: 1.41792 | acc: 0.31362 | val_loss: 1.72188 | val_acc: 0.25521\n",
      "Epoch[13/500], | loss: 1.43530 | acc: 0.30878 | val_loss: 1.76015 | val_acc: 0.22917\n",
      "Epoch[14/500], | loss: 1.41089 | acc: 0.31510 | val_loss: 1.83846 | val_acc: 0.20833\n",
      "Epoch[15/500], | loss: 1.41220 | acc: 0.31771 | val_loss: 1.63287 | val_acc: 0.23438\n",
      "Epoch[16/500], | loss: 1.41675 | acc: 0.31324 | val_loss: 1.75887 | val_acc: 0.20833\n",
      "Epoch[17/500], | loss: 1.44116 | acc: 0.29576 | val_loss: 1.73752 | val_acc: 0.22396\n",
      "Epoch[18/500], | loss: 1.39037 | acc: 0.32664 | val_loss: 1.70213 | val_acc: 0.20833\n",
      "Epoch[19/500], | loss: 1.41106 | acc: 0.32254 | val_loss: 1.63974 | val_acc: 0.26042\n",
      "Epoch[20/500], | loss: 1.40501 | acc: 0.33222 | val_loss: 1.66982 | val_acc: 0.23438\n",
      "Epoch[21/500], | loss: 1.41196 | acc: 0.30506 | val_loss: 1.73483 | val_acc: 0.23438\n",
      "Epoch[22/500], | loss: 1.41702 | acc: 0.31622 | val_loss: 1.65271 | val_acc: 0.23958\n",
      "Epoch[23/500], | loss: 1.42786 | acc: 0.31771 | val_loss: 1.70953 | val_acc: 0.17708\n",
      "Epoch[24/500], | loss: 1.40463 | acc: 0.31548 | val_loss: 1.67441 | val_acc: 0.25521\n",
      "Epoch[25/500], | loss: 1.39891 | acc: 0.33408 | val_loss: 1.82186 | val_acc: 0.15625\n",
      "Epoch[26/500], | loss: 1.42590 | acc: 0.31138 | val_loss: 1.73966 | val_acc: 0.20312\n",
      "Epoch[27/500], | loss: 1.42231 | acc: 0.30952 | val_loss: 1.69344 | val_acc: 0.25521\n",
      "Epoch[28/500], | loss: 1.40515 | acc: 0.31176 | val_loss: 1.65710 | val_acc: 0.28125\n",
      "Epoch[29/500], | loss: 1.39594 | acc: 0.32329 | val_loss: 1.64083 | val_acc: 0.25521\n",
      "Epoch[30/500], | loss: 1.38667 | acc: 0.32180 | val_loss: 1.67345 | val_acc: 0.26042\n",
      "Epoch[31/500], | loss: 1.39779 | acc: 0.32068 | val_loss: 1.69357 | val_acc: 0.22917\n",
      "Epoch[32/500], | loss: 1.41321 | acc: 0.30208 | val_loss: 1.65659 | val_acc: 0.23438\n",
      "Epoch[33/500], | loss: 1.38976 | acc: 0.32143 | val_loss: 1.62818 | val_acc: 0.22917\n",
      "Epoch[34/500], | loss: 1.38448 | acc: 0.32626 | val_loss: 1.69600 | val_acc: 0.27083\n",
      "Epoch[35/500], | loss: 1.39906 | acc: 0.32143 | val_loss: 1.70461 | val_acc: 0.22917\n",
      "Epoch[36/500], | loss: 1.39391 | acc: 0.31548 | val_loss: 1.66680 | val_acc: 0.25521\n",
      "Epoch[37/500], | loss: 1.38526 | acc: 0.32403 | val_loss: 1.71054 | val_acc: 0.23958\n",
      "Epoch[38/500], | loss: 1.41019 | acc: 0.29650 | val_loss: 1.78917 | val_acc: 0.19271\n",
      "Epoch[39/500], | loss: 1.38208 | acc: 0.32626 | val_loss: 1.74297 | val_acc: 0.23438\n",
      "Epoch[40/500], | loss: 1.39162 | acc: 0.32440 | val_loss: 1.65104 | val_acc: 0.25000\n",
      "Epoch[41/500], | loss: 1.38871 | acc: 0.32664 | val_loss: 1.66210 | val_acc: 0.28125\n",
      "Epoch[42/500], | loss: 1.39077 | acc: 0.32589 | val_loss: 1.70625 | val_acc: 0.18750\n",
      "Epoch[43/500], | loss: 1.38931 | acc: 0.32403 | val_loss: 1.70463 | val_acc: 0.19792\n",
      "Epoch[44/500], | loss: 1.38800 | acc: 0.32180 | val_loss: 1.67005 | val_acc: 0.21875\n",
      "Epoch[45/500], | loss: 1.37853 | acc: 0.32552 | val_loss: 1.68613 | val_acc: 0.21875\n",
      "Epoch[46/500], | loss: 1.39604 | acc: 0.32515 | val_loss: 1.54608 | val_acc: 0.29688\n",
      "Epoch[47/500], | loss: 1.38057 | acc: 0.32999 | val_loss: 1.71052 | val_acc: 0.22917\n",
      "Epoch[48/500], | loss: 1.38630 | acc: 0.33780 | val_loss: 1.62364 | val_acc: 0.23958\n",
      "Epoch[49/500], | loss: 1.37722 | acc: 0.31101 | val_loss: 1.62374 | val_acc: 0.23958\n",
      "Epoch[50/500], | loss: 1.38839 | acc: 0.32403 | val_loss: 1.65410 | val_acc: 0.25000\n",
      "Epoch[51/500], | loss: 1.36541 | acc: 0.32292 | val_loss: 1.60932 | val_acc: 0.25000\n",
      "Epoch[52/500], | loss: 1.37155 | acc: 0.32292 | val_loss: 1.69724 | val_acc: 0.23958\n",
      "Epoch[53/500], | loss: 1.38106 | acc: 0.33036 | val_loss: 1.68586 | val_acc: 0.22396\n",
      "Epoch[54/500], | loss: 1.38298 | acc: 0.31994 | val_loss: 1.69794 | val_acc: 0.24479\n",
      "Epoch[55/500], | loss: 1.39837 | acc: 0.31808 | val_loss: 1.70592 | val_acc: 0.26042\n",
      "Epoch[56/500], | loss: 1.37136 | acc: 0.33371 | val_loss: 1.71466 | val_acc: 0.20833\n",
      "Epoch[57/500], | loss: 1.37142 | acc: 0.33259 | val_loss: 1.63092 | val_acc: 0.24479\n",
      "Epoch[58/500], | loss: 1.36948 | acc: 0.33185 | val_loss: 1.54915 | val_acc: 0.26042\n",
      "Epoch[59/500], | loss: 1.38930 | acc: 0.33110 | val_loss: 1.67277 | val_acc: 0.22917\n",
      "Epoch[60/500], | loss: 1.36436 | acc: 0.32775 | val_loss: 1.52105 | val_acc: 0.29167\n",
      "Epoch[61/500], | loss: 1.37291 | acc: 0.33036 | val_loss: 1.65502 | val_acc: 0.20312\n",
      "Epoch[62/500], | loss: 1.38954 | acc: 0.32366 | val_loss: 1.63698 | val_acc: 0.22396\n",
      "Epoch[63/500], | loss: 1.38330 | acc: 0.31548 | val_loss: 1.61097 | val_acc: 0.24479\n",
      "Epoch[64/500], | loss: 1.36694 | acc: 0.32701 | val_loss: 1.60946 | val_acc: 0.24479\n",
      "Epoch[65/500], | loss: 1.38210 | acc: 0.30246 | val_loss: 1.66319 | val_acc: 0.21354\n",
      "Epoch[66/500], | loss: 1.38180 | acc: 0.32031 | val_loss: 1.70480 | val_acc: 0.22396\n",
      "Epoch[67/500], | loss: 1.36545 | acc: 0.32329 | val_loss: 1.61231 | val_acc: 0.25521\n",
      "Epoch[68/500], | loss: 1.36655 | acc: 0.31882 | val_loss: 1.67169 | val_acc: 0.25521\n",
      "Epoch[69/500], | loss: 1.37958 | acc: 0.32292 | val_loss: 1.70354 | val_acc: 0.21354\n",
      "Epoch[70/500], | loss: 1.38529 | acc: 0.32887 | val_loss: 1.67861 | val_acc: 0.23438\n",
      "Epoch[71/500], | loss: 1.35412 | acc: 0.34561 | val_loss: 1.65495 | val_acc: 0.25000\n",
      "Epoch[72/500], | loss: 1.38011 | acc: 0.31510 | val_loss: 1.66032 | val_acc: 0.25521\n",
      "Epoch[73/500], | loss: 1.37020 | acc: 0.32552 | val_loss: 1.69834 | val_acc: 0.20833\n",
      "Epoch[74/500], | loss: 1.37025 | acc: 0.31064 | val_loss: 1.54401 | val_acc: 0.24479\n",
      "Epoch[75/500], | loss: 1.39671 | acc: 0.31548 | val_loss: 1.72635 | val_acc: 0.22396\n",
      "Epoch[76/500], | loss: 1.36407 | acc: 0.33036 | val_loss: 1.65400 | val_acc: 0.23438\n",
      "Epoch[77/500], | loss: 1.37000 | acc: 0.31548 | val_loss: 1.64258 | val_acc: 0.26042\n",
      "Epoch[78/500], | loss: 1.36787 | acc: 0.33036 | val_loss: 1.63531 | val_acc: 0.27083\n",
      "Epoch[79/500], | loss: 1.36605 | acc: 0.33371 | val_loss: 1.61413 | val_acc: 0.29688\n",
      "Epoch[80/500], | loss: 1.37137 | acc: 0.31659 | val_loss: 1.59398 | val_acc: 0.22917\n",
      "Epoch[81/500], | loss: 1.36382 | acc: 0.33966 | val_loss: 1.71242 | val_acc: 0.17708\n",
      "Epoch[82/500], | loss: 1.37332 | acc: 0.32106 | val_loss: 1.57564 | val_acc: 0.26042\n",
      "Epoch[83/500], | loss: 1.37264 | acc: 0.33036 | val_loss: 1.62286 | val_acc: 0.21875\n",
      "Epoch[84/500], | loss: 1.39469 | acc: 0.31585 | val_loss: 1.62285 | val_acc: 0.25000\n",
      "Epoch[85/500], | loss: 1.37332 | acc: 0.32961 | val_loss: 1.68719 | val_acc: 0.17188\n",
      "Epoch[86/500], | loss: 1.37127 | acc: 0.32701 | val_loss: 1.65537 | val_acc: 0.27083\n",
      "Epoch[87/500], | loss: 1.34789 | acc: 0.34375 | val_loss: 1.55142 | val_acc: 0.28646\n",
      "Epoch[88/500], | loss: 1.36345 | acc: 0.31994 | val_loss: 1.58697 | val_acc: 0.21875\n",
      "Epoch[89/500], | loss: 1.36456 | acc: 0.32068 | val_loss: 1.51867 | val_acc: 0.27083\n",
      "Epoch[90/500], | loss: 1.34956 | acc: 0.34449 | val_loss: 1.59534 | val_acc: 0.25000\n",
      "Epoch[91/500], | loss: 1.37540 | acc: 0.34226 | val_loss: 1.53937 | val_acc: 0.25000\n",
      "Epoch[92/500], | loss: 1.36273 | acc: 0.32999 | val_loss: 1.62551 | val_acc: 0.20312\n",
      "Epoch[93/500], | loss: 1.36017 | acc: 0.32961 | val_loss: 1.70263 | val_acc: 0.21875\n",
      "Epoch[94/500], | loss: 1.36351 | acc: 0.33594 | val_loss: 1.73065 | val_acc: 0.22917\n",
      "Epoch[95/500], | loss: 1.37103 | acc: 0.31324 | val_loss: 1.57149 | val_acc: 0.23958\n",
      "Epoch[96/500], | loss: 1.35723 | acc: 0.33854 | val_loss: 1.64742 | val_acc: 0.26042\n",
      "Epoch[97/500], | loss: 1.35970 | acc: 0.32180 | val_loss: 1.69362 | val_acc: 0.19271\n",
      "Epoch[98/500], | loss: 1.37227 | acc: 0.32106 | val_loss: 1.66164 | val_acc: 0.21354\n",
      "Epoch[99/500], | loss: 1.36001 | acc: 0.32254 | val_loss: 1.59509 | val_acc: 0.27604\n",
      "Epoch[100/500], | loss: 1.35265 | acc: 0.34115 | val_loss: 1.62075 | val_acc: 0.25521\n",
      "Epoch[101/500], | loss: 1.36824 | acc: 0.32292 | val_loss: 1.61873 | val_acc: 0.27604\n",
      "Epoch[102/500], | loss: 1.35472 | acc: 0.33705 | val_loss: 1.63461 | val_acc: 0.27604\n",
      "Epoch[103/500], | loss: 1.35227 | acc: 0.33185 | val_loss: 1.64139 | val_acc: 0.20833\n",
      "Epoch[104/500], | loss: 1.34829 | acc: 0.33222 | val_loss: 1.58563 | val_acc: 0.25521\n",
      "Epoch[105/500], | loss: 1.36456 | acc: 0.33482 | val_loss: 1.55859 | val_acc: 0.22917\n",
      "Epoch[106/500], | loss: 1.36605 | acc: 0.32217 | val_loss: 1.64403 | val_acc: 0.21354\n",
      "Epoch[107/500], | loss: 1.34523 | acc: 0.33519 | val_loss: 1.58647 | val_acc: 0.21875\n",
      "Epoch[108/500], | loss: 1.35889 | acc: 0.33891 | val_loss: 1.62047 | val_acc: 0.20312\n",
      "Epoch[109/500], | loss: 1.35638 | acc: 0.34115 | val_loss: 1.68052 | val_acc: 0.19792\n",
      "Epoch[110/500], | loss: 1.36119 | acc: 0.32812 | val_loss: 1.68816 | val_acc: 0.19792\n",
      "Epoch[111/500], | loss: 1.37492 | acc: 0.31548 | val_loss: 1.68396 | val_acc: 0.21354\n",
      "Epoch[112/500], | loss: 1.34851 | acc: 0.33891 | val_loss: 1.64842 | val_acc: 0.23438\n",
      "Epoch[113/500], | loss: 1.35359 | acc: 0.33408 | val_loss: 1.63051 | val_acc: 0.26042\n",
      "Epoch[114/500], | loss: 1.34948 | acc: 0.33817 | val_loss: 1.57796 | val_acc: 0.23438\n",
      "Epoch[115/500], | loss: 1.36153 | acc: 0.32887 | val_loss: 1.61078 | val_acc: 0.27604\n",
      "Epoch[116/500], | loss: 1.35851 | acc: 0.32999 | val_loss: 1.60379 | val_acc: 0.23958\n",
      "Epoch[117/500], | loss: 1.33972 | acc: 0.33296 | val_loss: 1.57350 | val_acc: 0.23438\n",
      "Epoch[118/500], | loss: 1.34153 | acc: 0.34077 | val_loss: 1.60629 | val_acc: 0.26042\n",
      "Epoch[119/500], | loss: 1.35024 | acc: 0.32589 | val_loss: 1.58709 | val_acc: 0.22396\n",
      "Epoch[120/500], | loss: 1.34433 | acc: 0.34040 | val_loss: 1.59841 | val_acc: 0.19792\n",
      "Epoch[121/500], | loss: 1.36829 | acc: 0.33631 | val_loss: 1.56387 | val_acc: 0.26562\n",
      "Epoch[122/500], | loss: 1.34257 | acc: 0.33110 | val_loss: 1.63488 | val_acc: 0.24479\n",
      "Epoch[123/500], | loss: 1.36497 | acc: 0.31399 | val_loss: 1.62456 | val_acc: 0.22917\n",
      "Epoch[124/500], | loss: 1.36021 | acc: 0.32738 | val_loss: 1.66490 | val_acc: 0.18750\n",
      "Epoch[125/500], | loss: 1.33068 | acc: 0.33705 | val_loss: 1.64259 | val_acc: 0.26042\n",
      "Epoch[126/500], | loss: 1.34107 | acc: 0.33296 | val_loss: 1.60420 | val_acc: 0.23958\n",
      "Epoch[127/500], | loss: 1.33820 | acc: 0.33259 | val_loss: 1.54599 | val_acc: 0.23958\n",
      "Epoch[128/500], | loss: 1.32934 | acc: 0.34524 | val_loss: 1.68769 | val_acc: 0.22917\n",
      "Epoch[129/500], | loss: 1.34524 | acc: 0.33631 | val_loss: 1.56448 | val_acc: 0.26042\n",
      "Epoch[130/500], | loss: 1.33864 | acc: 0.33594 | val_loss: 1.60730 | val_acc: 0.26562\n",
      "Epoch[131/500], | loss: 1.34939 | acc: 0.34077 | val_loss: 1.68884 | val_acc: 0.22396\n",
      "Epoch[132/500], | loss: 1.33796 | acc: 0.34375 | val_loss: 1.63757 | val_acc: 0.26042\n",
      "Epoch[133/500], | loss: 1.34675 | acc: 0.34338 | val_loss: 1.53384 | val_acc: 0.21354\n",
      "Epoch[134/500], | loss: 1.31779 | acc: 0.36384 | val_loss: 1.66021 | val_acc: 0.22917\n",
      "Epoch[135/500], | loss: 1.34544 | acc: 0.33929 | val_loss: 1.63802 | val_acc: 0.27083\n",
      "Epoch[136/500], | loss: 1.34982 | acc: 0.34226 | val_loss: 1.66281 | val_acc: 0.23438\n",
      "Epoch[137/500], | loss: 1.35288 | acc: 0.33259 | val_loss: 1.77865 | val_acc: 0.16667\n",
      "Epoch[138/500], | loss: 1.36230 | acc: 0.33594 | val_loss: 1.63750 | val_acc: 0.25521\n",
      "Epoch[139/500], | loss: 1.33820 | acc: 0.33371 | val_loss: 1.57660 | val_acc: 0.27604\n",
      "Epoch[140/500], | loss: 1.35571 | acc: 0.33147 | val_loss: 1.62559 | val_acc: 0.29688\n",
      "Epoch[141/500], | loss: 1.35016 | acc: 0.32738 | val_loss: 1.57816 | val_acc: 0.27083\n",
      "Epoch[142/500], | loss: 1.34460 | acc: 0.33371 | val_loss: 1.60318 | val_acc: 0.27083\n",
      "Epoch[143/500], | loss: 1.33521 | acc: 0.35491 | val_loss: 1.66348 | val_acc: 0.19271\n",
      "Epoch[144/500], | loss: 1.34590 | acc: 0.35454 | val_loss: 1.63452 | val_acc: 0.22917\n",
      "Epoch[145/500], | loss: 1.34187 | acc: 0.33966 | val_loss: 1.53702 | val_acc: 0.27604\n",
      "Epoch[146/500], | loss: 1.32690 | acc: 0.34673 | val_loss: 1.48991 | val_acc: 0.28125\n",
      "Epoch[147/500], | loss: 1.33939 | acc: 0.33817 | val_loss: 1.59592 | val_acc: 0.28646\n",
      "Epoch[148/500], | loss: 1.32955 | acc: 0.34449 | val_loss: 1.69225 | val_acc: 0.25521\n",
      "Epoch[149/500], | loss: 1.33150 | acc: 0.33036 | val_loss: 1.67683 | val_acc: 0.22396\n",
      "Epoch[150/500], | loss: 1.34311 | acc: 0.33222 | val_loss: 1.65722 | val_acc: 0.21875\n",
      "Epoch[151/500], | loss: 1.34092 | acc: 0.34115 | val_loss: 1.64551 | val_acc: 0.26562\n",
      "Epoch[152/500], | loss: 1.33148 | acc: 0.35677 | val_loss: 1.65287 | val_acc: 0.21354\n",
      "Epoch[153/500], | loss: 1.32804 | acc: 0.33929 | val_loss: 1.63783 | val_acc: 0.16667\n",
      "Epoch[154/500], | loss: 1.33872 | acc: 0.34821 | val_loss: 1.64664 | val_acc: 0.24479\n",
      "Epoch[155/500], | loss: 1.33421 | acc: 0.33408 | val_loss: 1.64344 | val_acc: 0.27083\n",
      "Epoch[156/500], | loss: 1.34268 | acc: 0.33333 | val_loss: 1.59468 | val_acc: 0.27604\n",
      "Epoch[157/500], | loss: 1.33514 | acc: 0.34487 | val_loss: 1.54407 | val_acc: 0.23958\n",
      "Epoch[158/500], | loss: 1.34407 | acc: 0.32887 | val_loss: 1.62223 | val_acc: 0.26562\n",
      "Epoch[159/500], | loss: 1.33674 | acc: 0.33705 | val_loss: 1.52802 | val_acc: 0.28646\n",
      "Epoch[160/500], | loss: 1.33746 | acc: 0.34896 | val_loss: 1.52064 | val_acc: 0.29167\n",
      "Epoch[161/500], | loss: 1.32392 | acc: 0.35677 | val_loss: 1.68396 | val_acc: 0.23958\n",
      "Epoch[162/500], | loss: 1.31949 | acc: 0.34598 | val_loss: 1.59080 | val_acc: 0.28646\n",
      "Epoch[163/500], | loss: 1.33132 | acc: 0.34077 | val_loss: 1.47500 | val_acc: 0.31771\n",
      "Epoch[164/500], | loss: 1.31103 | acc: 0.36049 | val_loss: 1.62879 | val_acc: 0.19271\n",
      "Epoch[165/500], | loss: 1.34065 | acc: 0.33296 | val_loss: 1.64536 | val_acc: 0.22917\n",
      "Epoch[166/500], | loss: 1.34115 | acc: 0.33854 | val_loss: 1.60145 | val_acc: 0.23438\n",
      "Epoch[167/500], | loss: 1.32637 | acc: 0.35975 | val_loss: 1.58006 | val_acc: 0.24479\n",
      "Epoch[168/500], | loss: 1.34896 | acc: 0.33705 | val_loss: 1.69533 | val_acc: 0.22917\n",
      "Epoch[169/500], | loss: 1.33998 | acc: 0.34933 | val_loss: 1.64173 | val_acc: 0.17188\n",
      "Epoch[170/500], | loss: 1.33424 | acc: 0.34859 | val_loss: 1.56541 | val_acc: 0.28646\n",
      "Epoch[171/500], | loss: 1.33180 | acc: 0.33929 | val_loss: 1.59708 | val_acc: 0.22917\n",
      "Epoch[172/500], | loss: 1.32584 | acc: 0.34635 | val_loss: 1.60743 | val_acc: 0.26042\n",
      "Epoch[173/500], | loss: 1.33135 | acc: 0.34673 | val_loss: 1.60694 | val_acc: 0.22917\n",
      "Epoch[174/500], | loss: 1.32681 | acc: 0.35082 | val_loss: 1.63001 | val_acc: 0.19271\n",
      "Epoch[175/500], | loss: 1.34988 | acc: 0.32292 | val_loss: 1.61807 | val_acc: 0.27083\n",
      "Epoch[176/500], | loss: 1.32267 | acc: 0.35342 | val_loss: 1.58045 | val_acc: 0.29167\n",
      "Epoch[177/500], | loss: 1.34264 | acc: 0.33743 | val_loss: 1.59208 | val_acc: 0.25000\n",
      "Epoch[178/500], | loss: 1.32903 | acc: 0.35342 | val_loss: 1.57896 | val_acc: 0.23438\n",
      "Epoch[179/500], | loss: 1.32513 | acc: 0.34524 | val_loss: 1.61218 | val_acc: 0.25000\n",
      "Epoch[180/500], | loss: 1.32241 | acc: 0.33966 | val_loss: 1.55435 | val_acc: 0.27083\n",
      "Epoch[181/500], | loss: 1.32622 | acc: 0.36086 | val_loss: 1.56945 | val_acc: 0.20833\n",
      "Epoch[182/500], | loss: 1.31161 | acc: 0.35417 | val_loss: 1.60417 | val_acc: 0.25521\n",
      "Epoch[183/500], | loss: 1.32218 | acc: 0.36272 | val_loss: 1.60162 | val_acc: 0.22396\n",
      "Epoch[184/500], | loss: 1.33952 | acc: 0.34561 | val_loss: 1.60225 | val_acc: 0.23958\n",
      "Epoch[185/500], | loss: 1.33580 | acc: 0.34189 | val_loss: 1.61946 | val_acc: 0.26562\n",
      "Epoch[186/500], | loss: 1.31364 | acc: 0.35119 | val_loss: 1.61698 | val_acc: 0.23438\n",
      "Epoch[187/500], | loss: 1.33326 | acc: 0.33705 | val_loss: 1.54386 | val_acc: 0.24479\n",
      "Epoch[188/500], | loss: 1.32640 | acc: 0.35082 | val_loss: 1.58547 | val_acc: 0.27083\n",
      "Epoch[189/500], | loss: 1.30732 | acc: 0.36496 | val_loss: 1.65901 | val_acc: 0.24479\n",
      "Epoch[190/500], | loss: 1.31446 | acc: 0.35714 | val_loss: 1.57126 | val_acc: 0.26562\n",
      "Epoch[191/500], | loss: 1.31390 | acc: 0.36161 | val_loss: 1.67178 | val_acc: 0.21875\n",
      "Epoch[192/500], | loss: 1.31893 | acc: 0.35491 | val_loss: 1.68876 | val_acc: 0.21354\n",
      "Epoch[193/500], | loss: 1.31461 | acc: 0.35789 | val_loss: 1.52472 | val_acc: 0.28646\n",
      "Epoch[194/500], | loss: 1.31862 | acc: 0.34375 | val_loss: 1.67286 | val_acc: 0.22917\n",
      "Epoch[195/500], | loss: 1.30875 | acc: 0.36198 | val_loss: 1.61723 | val_acc: 0.20833\n",
      "Epoch[196/500], | loss: 1.30865 | acc: 0.35714 | val_loss: 1.65808 | val_acc: 0.21875\n",
      "Epoch[197/500], | loss: 1.32021 | acc: 0.34710 | val_loss: 1.58841 | val_acc: 0.27083\n",
      "Epoch[198/500], | loss: 1.34016 | acc: 0.34821 | val_loss: 1.61931 | val_acc: 0.23438\n",
      "Epoch[199/500], | loss: 1.34546 | acc: 0.33110 | val_loss: 1.52872 | val_acc: 0.25000\n",
      "Epoch[200/500], | loss: 1.32171 | acc: 0.34226 | val_loss: 1.58954 | val_acc: 0.24479\n",
      "Epoch[201/500], | loss: 1.31033 | acc: 0.35007 | val_loss: 1.54845 | val_acc: 0.23438\n",
      "Epoch[202/500], | loss: 1.32641 | acc: 0.35379 | val_loss: 1.58175 | val_acc: 0.21875\n",
      "Epoch[203/500], | loss: 1.32040 | acc: 0.34784 | val_loss: 1.54284 | val_acc: 0.28125\n",
      "Epoch[204/500], | loss: 1.33254 | acc: 0.33780 | val_loss: 1.55745 | val_acc: 0.25000\n",
      "Epoch[205/500], | loss: 1.33121 | acc: 0.35268 | val_loss: 1.63798 | val_acc: 0.22917\n",
      "Epoch[206/500], | loss: 1.32336 | acc: 0.34226 | val_loss: 1.55380 | val_acc: 0.27083\n",
      "Epoch[207/500], | loss: 1.32795 | acc: 0.34263 | val_loss: 1.65053 | val_acc: 0.22396\n",
      "Epoch[208/500], | loss: 1.32455 | acc: 0.34189 | val_loss: 1.54410 | val_acc: 0.26042\n",
      "Epoch[209/500], | loss: 1.31809 | acc: 0.36235 | val_loss: 1.51002 | val_acc: 0.24479\n",
      "Epoch[210/500], | loss: 1.33164 | acc: 0.34152 | val_loss: 1.59061 | val_acc: 0.29167\n",
      "Epoch[211/500], | loss: 1.33495 | acc: 0.34040 | val_loss: 1.63091 | val_acc: 0.22396\n",
      "Epoch[212/500], | loss: 1.32246 | acc: 0.35342 | val_loss: 1.57905 | val_acc: 0.18750\n",
      "Epoch[213/500], | loss: 1.32747 | acc: 0.34263 | val_loss: 1.61943 | val_acc: 0.24479\n",
      "Epoch[214/500], | loss: 1.32076 | acc: 0.35007 | val_loss: 1.56173 | val_acc: 0.27604\n",
      "Epoch[215/500], | loss: 1.32021 | acc: 0.34487 | val_loss: 1.50283 | val_acc: 0.30208\n",
      "Epoch[216/500], | loss: 1.30894 | acc: 0.36161 | val_loss: 1.55746 | val_acc: 0.26042\n",
      "Epoch[217/500], | loss: 1.31214 | acc: 0.36384 | val_loss: 1.57409 | val_acc: 0.25521\n",
      "Epoch[218/500], | loss: 1.32929 | acc: 0.35045 | val_loss: 1.61979 | val_acc: 0.20833\n",
      "Epoch[219/500], | loss: 1.31651 | acc: 0.35007 | val_loss: 1.59541 | val_acc: 0.28125\n",
      "Epoch[220/500], | loss: 1.33034 | acc: 0.34896 | val_loss: 1.57848 | val_acc: 0.23438\n",
      "Epoch[221/500], | loss: 1.31676 | acc: 0.36607 | val_loss: 1.68638 | val_acc: 0.25000\n",
      "Epoch[222/500], | loss: 1.32125 | acc: 0.35379 | val_loss: 1.49597 | val_acc: 0.27604\n",
      "Epoch[223/500], | loss: 1.32587 | acc: 0.35789 | val_loss: 1.63119 | val_acc: 0.22917\n",
      "Epoch[224/500], | loss: 1.30230 | acc: 0.35082 | val_loss: 1.66400 | val_acc: 0.21875\n",
      "Epoch[225/500], | loss: 1.31817 | acc: 0.34933 | val_loss: 1.52614 | val_acc: 0.23958\n",
      "Epoch[226/500], | loss: 1.30376 | acc: 0.36310 | val_loss: 1.58344 | val_acc: 0.27604\n",
      "Epoch[227/500], | loss: 1.31460 | acc: 0.35454 | val_loss: 1.60147 | val_acc: 0.22917\n",
      "Epoch[228/500], | loss: 1.30518 | acc: 0.35863 | val_loss: 1.54167 | val_acc: 0.30729\n",
      "Epoch[229/500], | loss: 1.31248 | acc: 0.36458 | val_loss: 1.47414 | val_acc: 0.28125\n",
      "Epoch[230/500], | loss: 1.32007 | acc: 0.35826 | val_loss: 1.61023 | val_acc: 0.22396\n",
      "Epoch[231/500], | loss: 1.30848 | acc: 0.35789 | val_loss: 1.55343 | val_acc: 0.28646\n",
      "Epoch[232/500], | loss: 1.32437 | acc: 0.34301 | val_loss: 1.58454 | val_acc: 0.28646\n",
      "Epoch[233/500], | loss: 1.30996 | acc: 0.35491 | val_loss: 1.55843 | val_acc: 0.25521\n",
      "Epoch[234/500], | loss: 1.31005 | acc: 0.36421 | val_loss: 1.53009 | val_acc: 0.25000\n",
      "Epoch[235/500], | loss: 1.30444 | acc: 0.36607 | val_loss: 1.55480 | val_acc: 0.29688\n",
      "Epoch[236/500], | loss: 1.30723 | acc: 0.35789 | val_loss: 1.51228 | val_acc: 0.22396\n",
      "Epoch[237/500], | loss: 1.31447 | acc: 0.35379 | val_loss: 1.62313 | val_acc: 0.25521\n",
      "Epoch[238/500], | loss: 1.31122 | acc: 0.35603 | val_loss: 1.53578 | val_acc: 0.30729\n",
      "Epoch[239/500], | loss: 1.31776 | acc: 0.37016 | val_loss: 1.60978 | val_acc: 0.23958\n",
      "Epoch[240/500], | loss: 1.30863 | acc: 0.36384 | val_loss: 1.55900 | val_acc: 0.27604\n",
      "Epoch[241/500], | loss: 1.32263 | acc: 0.34598 | val_loss: 1.53158 | val_acc: 0.29167\n",
      "Epoch[242/500], | loss: 1.32666 | acc: 0.33891 | val_loss: 1.59980 | val_acc: 0.23958\n",
      "Epoch[243/500], | loss: 1.30995 | acc: 0.37165 | val_loss: 1.60677 | val_acc: 0.23958\n",
      "Epoch[244/500], | loss: 1.32282 | acc: 0.33036 | val_loss: 1.47262 | val_acc: 0.34375\n",
      "Epoch[245/500], | loss: 1.30996 | acc: 0.36719 | val_loss: 1.55979 | val_acc: 0.26562\n",
      "Epoch[246/500], | loss: 1.31867 | acc: 0.34933 | val_loss: 1.50232 | val_acc: 0.23438\n",
      "Epoch[247/500], | loss: 1.32429 | acc: 0.34598 | val_loss: 1.55226 | val_acc: 0.31771\n",
      "Epoch[248/500], | loss: 1.30163 | acc: 0.35231 | val_loss: 1.54154 | val_acc: 0.22396\n",
      "Epoch[249/500], | loss: 1.31904 | acc: 0.35454 | val_loss: 1.63128 | val_acc: 0.25000\n",
      "Epoch[250/500], | loss: 1.30900 | acc: 0.35975 | val_loss: 1.61208 | val_acc: 0.23958\n",
      "Epoch[251/500], | loss: 1.30865 | acc: 0.36310 | val_loss: 1.63945 | val_acc: 0.21875\n",
      "Epoch[252/500], | loss: 1.32074 | acc: 0.34710 | val_loss: 1.58176 | val_acc: 0.26042\n",
      "Epoch[253/500], | loss: 1.31238 | acc: 0.34635 | val_loss: 1.60844 | val_acc: 0.20833\n",
      "Epoch[254/500], | loss: 1.31862 | acc: 0.35045 | val_loss: 1.53377 | val_acc: 0.24479\n",
      "Epoch[255/500], | loss: 1.32738 | acc: 0.35528 | val_loss: 1.63751 | val_acc: 0.25000\n",
      "Epoch[256/500], | loss: 1.30090 | acc: 0.36384 | val_loss: 1.56793 | val_acc: 0.25000\n",
      "Epoch[257/500], | loss: 1.31076 | acc: 0.35751 | val_loss: 1.59124 | val_acc: 0.25521\n",
      "Epoch[258/500], | loss: 1.30755 | acc: 0.36012 | val_loss: 1.60372 | val_acc: 0.24479\n",
      "Epoch[259/500], | loss: 1.30796 | acc: 0.37351 | val_loss: 1.53299 | val_acc: 0.25521\n",
      "Epoch[260/500], | loss: 1.31482 | acc: 0.34598 | val_loss: 1.55299 | val_acc: 0.22917\n",
      "Epoch[261/500], | loss: 1.30562 | acc: 0.35603 | val_loss: 1.56807 | val_acc: 0.26042\n",
      "Epoch[262/500], | loss: 1.31788 | acc: 0.35417 | val_loss: 1.54329 | val_acc: 0.29167\n",
      "Epoch[263/500], | loss: 1.29480 | acc: 0.36830 | val_loss: 1.62316 | val_acc: 0.27083\n",
      "Epoch[264/500], | loss: 1.30320 | acc: 0.35193 | val_loss: 1.60973 | val_acc: 0.22396\n",
      "Epoch[265/500], | loss: 1.30359 | acc: 0.35603 | val_loss: 1.55689 | val_acc: 0.26562\n",
      "Epoch[266/500], | loss: 1.31112 | acc: 0.35603 | val_loss: 1.51443 | val_acc: 0.25521\n",
      "Epoch[267/500], | loss: 1.30453 | acc: 0.36086 | val_loss: 1.59004 | val_acc: 0.24479\n",
      "Epoch[268/500], | loss: 1.31182 | acc: 0.34970 | val_loss: 1.54394 | val_acc: 0.23958\n",
      "Epoch[269/500], | loss: 1.31615 | acc: 0.35231 | val_loss: 1.60713 | val_acc: 0.20833\n",
      "Epoch[270/500], | loss: 1.31942 | acc: 0.35305 | val_loss: 1.60792 | val_acc: 0.23438\n",
      "Epoch[271/500], | loss: 1.32229 | acc: 0.34859 | val_loss: 1.59236 | val_acc: 0.26042\n",
      "Epoch[272/500], | loss: 1.32777 | acc: 0.35491 | val_loss: 1.54391 | val_acc: 0.26042\n",
      "Epoch[273/500], | loss: 1.31404 | acc: 0.34710 | val_loss: 1.55827 | val_acc: 0.26562\n",
      "Epoch[274/500], | loss: 1.31016 | acc: 0.35454 | val_loss: 1.52005 | val_acc: 0.28125\n",
      "Epoch[275/500], | loss: 1.32424 | acc: 0.36198 | val_loss: 1.63277 | val_acc: 0.26562\n",
      "Epoch[276/500], | loss: 1.30462 | acc: 0.36049 | val_loss: 1.52251 | val_acc: 0.24479\n",
      "Epoch[277/500], | loss: 1.30295 | acc: 0.37128 | val_loss: 1.51897 | val_acc: 0.26042\n",
      "Epoch[278/500], | loss: 1.31922 | acc: 0.35640 | val_loss: 1.56901 | val_acc: 0.26562\n",
      "Epoch[279/500], | loss: 1.31308 | acc: 0.36124 | val_loss: 1.51530 | val_acc: 0.26042\n",
      "Epoch[280/500], | loss: 1.31576 | acc: 0.36830 | val_loss: 1.55602 | val_acc: 0.24479\n",
      "Epoch[281/500], | loss: 1.31449 | acc: 0.35863 | val_loss: 1.54439 | val_acc: 0.25000\n",
      "Epoch[282/500], | loss: 1.30979 | acc: 0.34673 | val_loss: 1.51973 | val_acc: 0.26562\n",
      "Epoch[283/500], | loss: 1.31129 | acc: 0.36012 | val_loss: 1.51967 | val_acc: 0.27083\n",
      "Epoch[284/500], | loss: 1.30966 | acc: 0.36272 | val_loss: 1.54275 | val_acc: 0.29167\n",
      "Epoch[285/500], | loss: 1.31752 | acc: 0.35268 | val_loss: 1.57081 | val_acc: 0.27083\n",
      "Epoch[286/500], | loss: 1.31885 | acc: 0.36310 | val_loss: 1.53259 | val_acc: 0.25521\n",
      "Epoch[287/500], | loss: 1.30850 | acc: 0.35417 | val_loss: 1.56757 | val_acc: 0.21875\n",
      "Epoch[288/500], | loss: 1.31645 | acc: 0.35528 | val_loss: 1.57222 | val_acc: 0.27083\n",
      "Epoch[289/500], | loss: 1.28337 | acc: 0.37388 | val_loss: 1.54887 | val_acc: 0.18229\n",
      "Epoch[290/500], | loss: 1.31116 | acc: 0.35938 | val_loss: 1.58274 | val_acc: 0.25000\n",
      "Epoch[291/500], | loss: 1.31083 | acc: 0.35677 | val_loss: 1.50182 | val_acc: 0.31250\n",
      "Epoch[292/500], | loss: 1.30188 | acc: 0.36161 | val_loss: 1.52880 | val_acc: 0.27604\n",
      "Epoch[293/500], | loss: 1.29279 | acc: 0.36272 | val_loss: 1.48021 | val_acc: 0.24479\n",
      "Epoch[294/500], | loss: 1.30370 | acc: 0.36310 | val_loss: 1.53706 | val_acc: 0.29167\n",
      "Epoch[295/500], | loss: 1.30347 | acc: 0.36719 | val_loss: 1.48156 | val_acc: 0.34375\n",
      "Epoch[296/500], | loss: 1.31505 | acc: 0.37277 | val_loss: 1.56375 | val_acc: 0.28125\n",
      "Epoch[297/500], | loss: 1.29136 | acc: 0.36979 | val_loss: 1.53608 | val_acc: 0.28646\n",
      "Epoch[298/500], | loss: 1.31009 | acc: 0.36384 | val_loss: 1.59575 | val_acc: 0.23958\n",
      "Epoch[299/500], | loss: 1.30401 | acc: 0.37872 | val_loss: 1.55489 | val_acc: 0.23958\n",
      "Epoch[300/500], | loss: 1.30228 | acc: 0.36012 | val_loss: 1.58576 | val_acc: 0.24479\n",
      "Epoch[301/500], | loss: 1.30027 | acc: 0.38467 | val_loss: 1.51060 | val_acc: 0.22396\n",
      "Epoch[302/500], | loss: 1.29660 | acc: 0.36384 | val_loss: 1.53930 | val_acc: 0.23958\n",
      "Epoch[303/500], | loss: 1.29367 | acc: 0.36868 | val_loss: 1.57447 | val_acc: 0.23438\n",
      "Epoch[304/500], | loss: 1.30198 | acc: 0.36496 | val_loss: 1.58748 | val_acc: 0.24479\n",
      "Epoch[305/500], | loss: 1.29835 | acc: 0.36607 | val_loss: 1.52681 | val_acc: 0.29167\n",
      "Epoch[306/500], | loss: 1.31459 | acc: 0.36644 | val_loss: 1.51747 | val_acc: 0.22917\n",
      "Epoch[307/500], | loss: 1.29826 | acc: 0.36086 | val_loss: 1.58529 | val_acc: 0.25000\n",
      "Epoch[308/500], | loss: 1.30034 | acc: 0.36682 | val_loss: 1.53682 | val_acc: 0.22917\n",
      "Epoch[309/500], | loss: 1.30440 | acc: 0.36644 | val_loss: 1.62135 | val_acc: 0.20312\n",
      "Epoch[310/500], | loss: 1.30561 | acc: 0.37574 | val_loss: 1.51731 | val_acc: 0.25521\n",
      "Epoch[311/500], | loss: 1.30289 | acc: 0.37240 | val_loss: 1.55165 | val_acc: 0.23958\n",
      "Epoch[312/500], | loss: 1.28670 | acc: 0.38802 | val_loss: 1.61909 | val_acc: 0.25000\n",
      "Epoch[313/500], | loss: 1.29919 | acc: 0.37054 | val_loss: 1.52970 | val_acc: 0.24479\n",
      "Epoch[314/500], | loss: 1.30050 | acc: 0.36086 | val_loss: 1.51988 | val_acc: 0.25000\n",
      "Epoch[315/500], | loss: 1.30571 | acc: 0.36607 | val_loss: 1.52219 | val_acc: 0.26042\n",
      "Epoch[316/500], | loss: 1.30085 | acc: 0.38281 | val_loss: 1.59271 | val_acc: 0.23438\n",
      "Epoch[317/500], | loss: 1.31494 | acc: 0.36793 | val_loss: 1.57524 | val_acc: 0.21354\n",
      "Epoch[318/500], | loss: 1.29134 | acc: 0.37723 | val_loss: 1.51163 | val_acc: 0.29167\n",
      "Epoch[319/500], | loss: 1.30064 | acc: 0.36384 | val_loss: 1.59865 | val_acc: 0.29688\n",
      "Epoch[320/500], | loss: 1.30481 | acc: 0.36421 | val_loss: 1.60202 | val_acc: 0.18750\n",
      "Epoch[321/500], | loss: 1.31374 | acc: 0.35379 | val_loss: 1.59248 | val_acc: 0.26562\n",
      "Epoch[322/500], | loss: 1.29550 | acc: 0.37909 | val_loss: 1.56884 | val_acc: 0.23438\n",
      "Epoch[323/500], | loss: 1.31032 | acc: 0.35938 | val_loss: 1.57594 | val_acc: 0.23438\n",
      "Epoch[324/500], | loss: 1.31621 | acc: 0.35938 | val_loss: 1.57296 | val_acc: 0.24479\n",
      "Epoch[325/500], | loss: 1.29291 | acc: 0.38132 | val_loss: 1.48304 | val_acc: 0.32292\n",
      "Epoch[326/500], | loss: 1.29866 | acc: 0.37835 | val_loss: 1.47933 | val_acc: 0.26562\n",
      "Epoch[327/500], | loss: 1.30923 | acc: 0.37723 | val_loss: 1.62254 | val_acc: 0.22396\n",
      "Epoch[328/500], | loss: 1.31642 | acc: 0.35603 | val_loss: 1.49471 | val_acc: 0.23438\n",
      "Epoch[329/500], | loss: 1.29763 | acc: 0.37016 | val_loss: 1.56754 | val_acc: 0.22396\n",
      "Epoch[330/500], | loss: 1.31140 | acc: 0.36161 | val_loss: 1.54993 | val_acc: 0.24479\n",
      "Epoch[331/500], | loss: 1.29690 | acc: 0.37500 | val_loss: 1.55963 | val_acc: 0.25000\n",
      "Epoch[332/500], | loss: 1.30190 | acc: 0.37240 | val_loss: 1.55482 | val_acc: 0.27604\n",
      "Epoch[333/500], | loss: 1.29770 | acc: 0.37723 | val_loss: 1.58937 | val_acc: 0.30208\n",
      "Epoch[334/500], | loss: 1.30984 | acc: 0.37240 | val_loss: 1.48178 | val_acc: 0.27604\n",
      "Epoch[335/500], | loss: 1.30257 | acc: 0.38132 | val_loss: 1.53058 | val_acc: 0.26042\n",
      "Epoch[336/500], | loss: 1.29654 | acc: 0.37723 | val_loss: 1.51519 | val_acc: 0.22396\n",
      "Epoch[337/500], | loss: 1.29758 | acc: 0.37128 | val_loss: 1.53021 | val_acc: 0.25521\n",
      "Epoch[338/500], | loss: 1.29981 | acc: 0.38132 | val_loss: 1.47342 | val_acc: 0.25521\n",
      "Epoch[339/500], | loss: 1.28978 | acc: 0.37612 | val_loss: 1.49442 | val_acc: 0.25521\n",
      "Epoch[340/500], | loss: 1.31449 | acc: 0.35417 | val_loss: 1.52240 | val_acc: 0.25521\n",
      "Epoch[341/500], | loss: 1.29710 | acc: 0.36570 | val_loss: 1.53520 | val_acc: 0.16146\n",
      "Epoch[342/500], | loss: 1.28898 | acc: 0.37946 | val_loss: 1.54546 | val_acc: 0.22396\n",
      "Epoch[343/500], | loss: 1.31428 | acc: 0.36086 | val_loss: 1.50127 | val_acc: 0.25000\n",
      "Epoch[344/500], | loss: 1.30621 | acc: 0.37798 | val_loss: 1.46794 | val_acc: 0.24479\n",
      "Epoch[345/500], | loss: 1.30118 | acc: 0.36682 | val_loss: 1.51425 | val_acc: 0.27083\n",
      "Epoch[346/500], | loss: 1.30019 | acc: 0.37277 | val_loss: 1.63190 | val_acc: 0.27083\n",
      "Epoch[347/500], | loss: 1.29530 | acc: 0.38170 | val_loss: 1.45263 | val_acc: 0.30208\n",
      "Epoch[348/500], | loss: 1.29354 | acc: 0.37240 | val_loss: 1.47137 | val_acc: 0.25521\n",
      "Epoch[349/500], | loss: 1.29477 | acc: 0.38021 | val_loss: 1.55301 | val_acc: 0.27083\n",
      "Epoch[350/500], | loss: 1.30294 | acc: 0.38021 | val_loss: 1.49277 | val_acc: 0.31250\n",
      "Epoch[351/500], | loss: 1.29324 | acc: 0.37426 | val_loss: 1.58237 | val_acc: 0.26042\n",
      "Epoch[352/500], | loss: 1.30975 | acc: 0.36235 | val_loss: 1.52806 | val_acc: 0.25521\n",
      "Epoch[353/500], | loss: 1.27988 | acc: 0.38728 | val_loss: 1.57294 | val_acc: 0.21354\n",
      "Epoch[354/500], | loss: 1.30456 | acc: 0.36905 | val_loss: 1.50158 | val_acc: 0.24479\n",
      "Epoch[355/500], | loss: 1.30080 | acc: 0.35826 | val_loss: 1.54521 | val_acc: 0.25000\n",
      "Epoch[356/500], | loss: 1.28678 | acc: 0.38170 | val_loss: 1.55826 | val_acc: 0.25521\n",
      "Epoch[357/500], | loss: 1.30479 | acc: 0.37463 | val_loss: 1.55105 | val_acc: 0.19271\n",
      "Epoch[358/500], | loss: 1.30040 | acc: 0.37240 | val_loss: 1.53286 | val_acc: 0.29688\n",
      "Epoch[359/500], | loss: 1.30490 | acc: 0.35900 | val_loss: 1.55671 | val_acc: 0.24479\n",
      "Epoch[360/500], | loss: 1.28822 | acc: 0.38765 | val_loss: 1.57290 | val_acc: 0.26042\n",
      "Epoch[361/500], | loss: 1.30422 | acc: 0.36458 | val_loss: 1.52838 | val_acc: 0.24479\n",
      "Epoch[362/500], | loss: 1.29596 | acc: 0.37686 | val_loss: 1.49549 | val_acc: 0.27083\n",
      "Epoch[363/500], | loss: 1.29543 | acc: 0.39174 | val_loss: 1.55858 | val_acc: 0.26042\n",
      "Epoch[364/500], | loss: 1.30757 | acc: 0.36905 | val_loss: 1.50158 | val_acc: 0.28646\n",
      "Epoch[365/500], | loss: 1.29801 | acc: 0.37574 | val_loss: 1.58306 | val_acc: 0.20312\n",
      "Epoch[366/500], | loss: 1.31022 | acc: 0.36086 | val_loss: 1.54892 | val_acc: 0.26562\n",
      "Epoch[367/500], | loss: 1.27820 | acc: 0.38095 | val_loss: 1.49920 | val_acc: 0.29688\n",
      "Epoch[368/500], | loss: 1.29740 | acc: 0.36272 | val_loss: 1.52141 | val_acc: 0.25000\n",
      "Epoch[369/500], | loss: 1.28925 | acc: 0.37872 | val_loss: 1.56645 | val_acc: 0.26042\n",
      "Epoch[370/500], | loss: 1.28214 | acc: 0.37984 | val_loss: 1.56063 | val_acc: 0.29167\n",
      "Epoch[371/500], | loss: 1.28698 | acc: 0.38170 | val_loss: 1.55961 | val_acc: 0.26562\n",
      "Epoch[372/500], | loss: 1.30629 | acc: 0.35789 | val_loss: 1.52531 | val_acc: 0.24479\n",
      "Epoch[373/500], | loss: 1.28667 | acc: 0.36719 | val_loss: 1.59911 | val_acc: 0.22396\n",
      "Epoch[374/500], | loss: 1.28919 | acc: 0.37054 | val_loss: 1.55147 | val_acc: 0.33854\n",
      "Epoch[375/500], | loss: 1.29240 | acc: 0.36979 | val_loss: 1.48894 | val_acc: 0.25000\n",
      "Epoch[376/500], | loss: 1.28266 | acc: 0.39658 | val_loss: 1.57369 | val_acc: 0.24479\n",
      "Epoch[377/500], | loss: 1.29633 | acc: 0.37240 | val_loss: 1.55047 | val_acc: 0.29688\n",
      "Epoch[378/500], | loss: 1.29832 | acc: 0.37426 | val_loss: 1.55271 | val_acc: 0.22396\n",
      "Epoch[379/500], | loss: 1.29397 | acc: 0.37165 | val_loss: 1.58553 | val_acc: 0.18750\n",
      "Epoch[380/500], | loss: 1.29473 | acc: 0.37054 | val_loss: 1.60153 | val_acc: 0.25000\n",
      "Epoch[381/500], | loss: 1.29348 | acc: 0.36272 | val_loss: 1.57795 | val_acc: 0.23958\n",
      "Epoch[382/500], | loss: 1.28763 | acc: 0.36979 | val_loss: 1.57911 | val_acc: 0.23438\n",
      "Epoch[383/500], | loss: 1.28218 | acc: 0.37351 | val_loss: 1.55613 | val_acc: 0.23958\n",
      "Epoch[384/500], | loss: 1.28484 | acc: 0.38058 | val_loss: 1.56110 | val_acc: 0.23438\n",
      "Epoch[385/500], | loss: 1.27624 | acc: 0.38170 | val_loss: 1.52788 | val_acc: 0.27083\n",
      "Epoch[386/500], | loss: 1.29344 | acc: 0.37574 | val_loss: 1.63197 | val_acc: 0.25000\n",
      "Epoch[387/500], | loss: 1.30156 | acc: 0.35565 | val_loss: 1.55366 | val_acc: 0.27604\n",
      "Epoch[388/500], | loss: 1.29886 | acc: 0.35938 | val_loss: 1.49906 | val_acc: 0.28125\n",
      "Epoch[389/500], | loss: 1.29621 | acc: 0.36682 | val_loss: 1.57839 | val_acc: 0.22396\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[87], line 32\u001b[0m\n\u001b[0;32m     30\u001b[0m output \u001b[38;5;241m=\u001b[39m net(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m     31\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output, label)\n\u001b[1;32m---> 32\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     33\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     35\u001b[0m predicted \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(output, \u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[1;32mc:\\Miniconda3\\Lib\\site-packages\\torch\\_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    524\u001b[0m     )\n\u001b[1;32m--> 525\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[0;32m    526\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[0;32m    527\u001b[0m )\n",
      "File \u001b[1;32mc:\\Miniconda3\\Lib\\site-packages\\torch\\autograd\\__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 267\u001b[0m _engine_run_backward(\n\u001b[0;32m    268\u001b[0m     tensors,\n\u001b[0;32m    269\u001b[0m     grad_tensors_,\n\u001b[0;32m    270\u001b[0m     retain_graph,\n\u001b[0;32m    271\u001b[0m     create_graph,\n\u001b[0;32m    272\u001b[0m     inputs,\n\u001b[0;32m    273\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    274\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    275\u001b[0m )\n",
      "File \u001b[1;32mc:\\Miniconda3\\Lib\\site-packages\\torch\\autograd\\graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    745\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    746\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# モデルのインスタンス化\n",
    "net = resnet1d50(num_classes=4).to(device)\n",
    "\n",
    "# 誤差関数を交差エントロピーで計算\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# 最適化アルゴリズム\n",
    "optimizer = optim.Adam(net.parameters(), lr = 1e-6)\n",
    "\n",
    "# 学習\n",
    "n_epoch = 100\n",
    "for epoch in range(n_epoch):\n",
    "    # 精度と損失の初期化\n",
    "    train_acc, train_loss = 0, 0\n",
    "    val_acc, val_loss = 0, 0\n",
    "    n_train, n_test = 0, 0\n",
    "\n",
    "    # 学習\n",
    "    for train_input, train_label in train_loader:\n",
    "        n_train += len(train_label)\n",
    "\n",
    "        # 入力と正解ラベルをGPU上に移動\n",
    "        input = train_input.to(device)\n",
    "        label = train_label.to(device)\n",
    "        # print(f'input : {input.shape}, label : {label.shape}')\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = net(input)\n",
    "        loss = criterion(output, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        predicted = torch.max(output, 1)[1]\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        train_acc += (predicted == label).sum().item()\n",
    "\n",
    "    # 検証\n",
    "    for test_input, test_label in test_loader:\n",
    "        n_test += len(test_label)\n",
    "\n",
    "        test_input = test_input.to(device)\n",
    "        test_label = test_label.to(device)\n",
    "\n",
    "        test_output = net(test_input)\n",
    "        test_loss = criterion(test_output, test_label)\n",
    "\n",
    "        test_predicted = torch.max(test_output, 1)[1]\n",
    "\n",
    "        val_loss += test_loss.item()\n",
    "        val_acc += (test_predicted == test_label).sum().item()\n",
    "\n",
    "    # 精度を確率に変換\n",
    "    train_acc /= n_train\n",
    "    val_acc /= n_test\n",
    "    train_loss = train_loss * batch_size / n_train\n",
    "    val_loss = val_loss * batch_size / n_test\n",
    "\n",
    "    print(f\"Epoch[{epoch+1}/{n_epoch}], | loss: {train_loss:.5f} | acc: {train_acc:.5f} | val_loss: {val_loss:.5f} | val_acc: {val_acc:.5f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
