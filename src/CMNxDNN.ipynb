{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 実験概要\n",
    "## CMNによる特徴量抽出\n",
    "**ケプストラム平均正規化** (Cepstrum Mean Normalization : CMN) を使用し、特徴量を抽出する。\n",
    "切り出した波形に対して**離散フーリエ変換** (Discrete Fourier Transform : DFT) を行い、絶対値を取ることで**振幅スペクトル**を得る。\n",
    "これに対して対数を取って**対数振幅スペクトル**に変換し、逆離散フーリエ変換 (Inverse Discrete Fourier Transform : IDFT) を行うことで、ケプストラム領域へと変換し、先頭50要素を切り出し、結合した100要素の配列を前処理として返す。\n",
    "この前処理は\n",
    "```\n",
    "left, right, posture = pp.slicer(\"raw\\\\\" + tester.**.**.value)\n",
    "cepstrum = pp.cmn_denoise(left, right)\n",
    "```\n",
    "によって行われる。\n",
    "\n",
    "## DNNによる寝姿勢分類\n",
    "DNNによって前処理したデータから学習・分類を行う。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# モジュールのインポート"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "# 自作モジュール\n",
    "import datapath as dpath\n",
    "import preprocess as pp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataLoaderの定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tester以外のrawを読み込む\n",
    "for p in glob.glob(\".\\\\raw\\\\LMH\\\\*\\\\\", recursive=True):\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw\\LMH\\H002\\fl_center\n",
      "type : LMH,  tester : H002,  mattress : fl\n",
      "[WindowsPath('raw/LMH/H002/fl_center'), WindowsPath('raw/LMH/H003/fl_center'), WindowsPath('raw/LMH/L001/fl_center'), WindowsPath('raw/LMH/L003/fl_center'), WindowsPath('raw/LMH/M001/fls_center'), WindowsPath('raw/LMH/M001/fld_center'), WindowsPath('raw/LMH/M001/fls_center'), WindowsPath('raw/LMH/M002/fls_center'), WindowsPath('raw/LMH/M002/fld_center'), WindowsPath('raw/LMH/M002/fls_center'), WindowsPath('raw/LMH/M003/fls_center'), WindowsPath('raw/LMH/M003/fld_center'), WindowsPath('raw/LMH/M003/fls_center'), WindowsPath('raw/LMH/M004/fls_center'), WindowsPath('raw/LMH/M004/fld_center'), WindowsPath('raw/LMH/M004/fls_center'), WindowsPath('raw/LMH/H002/fl_center'), WindowsPath('raw/LMH/H003/fl_center'), WindowsPath('raw/LMH/L001/fl_center'), WindowsPath('raw/LMH/L003/fl_center'), WindowsPath('raw/LMH/M001/fls_center'), WindowsPath('raw/LMH/M001/fld_center'), WindowsPath('raw/LMH/M001/fls_center'), WindowsPath('raw/LMH/M002/fls_center'), WindowsPath('raw/LMH/M002/fld_center'), WindowsPath('raw/LMH/M002/fls_center'), WindowsPath('raw/LMH/M003/fls_center'), WindowsPath('raw/LMH/M003/fld_center'), WindowsPath('raw/LMH/M003/fls_center'), WindowsPath('raw/LMH/M004/fls_center'), WindowsPath('raw/LMH/M004/fld_center'), WindowsPath('raw/LMH/M004/fls_center')]\n"
     ]
    }
   ],
   "source": [
    "e = dpath.LMH.H002.value.fl_center\n",
    "print(e.value)\n",
    "\n",
    "type, tester, mattress = dpath.getattributes(e, position=False)\n",
    "print(f\"type : {type},  tester : {tester},  mattress : {mattress}\")\n",
    "train_paths = eval(f\"dpath.{type}.serch('{mattress}')\")\n",
    "print(train_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from typing import List, Tuple\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "# 学習用データセットを読み込むためのDataLoaderを定義\n",
    "class train_datasets(Dataset):\n",
    "    def __init__(self, identity, transforms=None):\n",
    "        self.transform = transforms\n",
    "        self.type, self.tester, self.mattress = dpath.getattributes(identity)\n",
    "        print(self.type, self.tester, self.mattress)\n",
    "        print(\"train\")\n",
    "\n",
    "        # tester以外のrawを読み込む\n",
    "        self.train_cepstrum = np.empty((0, 100))  # 100要素の配列\n",
    "        self.train_posture = np.empty(0) # 姿勢データ配列\n",
    "\n",
    "        # mattressに該当するrawのpathを読み込む\n",
    "        train_paths = eval(f\"dpath.{self.type}.serch('{self.mattress}')\")\n",
    "        for p in train_paths:\n",
    "            if identity.value == p:\n",
    "                continue\n",
    "            left, right, posture = pp.slicer(p)\n",
    "            cepstrum = pp.cmn_denoise(left, right)\n",
    "            self.train_cepstrum = np.append(self.train_cepstrum, cepstrum[:50], axis=0)\n",
    "            self.train_posture = np.append(self.train_posture, posture)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.train_posture)\n",
    "\n",
    "    def __getitem__(self, idx) -> Tuple[torch.tensor, torch.tensor]:\n",
    "        return self.train_cepstrum[idx], self.train_posture[idx]\n",
    "\n",
    "# テスト用データセットを読み込むためのDataLoaderを定義\n",
    "class test_datasets(Dataset):\n",
    "    def __init__(self, identity, transform=None):\n",
    "        self.transform = transform\n",
    "        self.type, self.tester, self.mattress = dpath.getattributes(identity)\n",
    "        print(\"test\")\n",
    "\n",
    "        # 前処理したrawを読み込む\n",
    "        self.left, self.right, self.posture = pp.slicer(identity.value)\n",
    "        self.cepstrum = pp.cmn_denoise(self.left, self.right)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.cepstrum)\n",
    "\n",
    "    def __getitem__(self, idx) -> Tuple[torch.tensor, torch.tensor]:\n",
    "        return self.cepstrum[idx], self.posture[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 学習＆検証"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデル定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# 各層のノード数を定義\n",
    "n_input = 100\n",
    "n_output = 4\n",
    "n_hidden = 128\n",
    "\n",
    "# 多層NNの定義\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, n_input, n_output, n_hidden):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(n_input, n_hidden)\n",
    "        self.layer2 = nn.Linear(n_hidden, n_hidden)\n",
    "        self.layer3 = nn.Linear(n_hidden, n_output)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.layer1(x))\n",
    "        x = self.relu(self.layer2(x))\n",
    "        x = self.layer3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LMH H002 st\n",
      "train\n",
      "data[340 / 403]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "all the input array dimensions except for the concatenation axis must match exactly, but along dimension 0, the array at index 0 has size 0 and the array at index 1 has size 50",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m identity \u001b[38;5;241m=\u001b[39m dpath\u001b[38;5;241m.\u001b[39mLMH\u001b[38;5;241m.\u001b[39mH002\u001b[38;5;241m.\u001b[39mvalue\u001b[38;5;241m.\u001b[39mst_center\n\u001b[1;32m----> 3\u001b[0m train \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_datasets\u001b[49m\u001b[43m(\u001b[49m\u001b[43midentity\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m test \u001b[38;5;241m=\u001b[39m test_datasets(identity)\n\u001b[0;32m      6\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m DataLoader(\n\u001b[0;32m      7\u001b[0m     train,\n\u001b[0;32m      8\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m,\n\u001b[0;32m      9\u001b[0m     shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     10\u001b[0m )\n",
      "Cell \u001b[1;32mIn[15], line 25\u001b[0m, in \u001b[0;36mtrain_datasets.__init__\u001b[1;34m(self, identity, transforms)\u001b[0m\n\u001b[0;32m     23\u001b[0m left, right, posture \u001b[38;5;241m=\u001b[39m pp\u001b[38;5;241m.\u001b[39mslicer(p)\n\u001b[0;32m     24\u001b[0m cepstrum \u001b[38;5;241m=\u001b[39m pp\u001b[38;5;241m.\u001b[39mcmn_denoise(left, right)\n\u001b[1;32m---> 25\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_cepstrum \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_cepstrum\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcepstrum\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_posture \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mvstack(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_posture, posture)\n",
      "File \u001b[1;32mc:\\Users\\satou\\anaconda3\\envs\\py311\\Lib\\site-packages\\numpy\\core\\shape_base.py:359\u001b[0m, in \u001b[0;36mhstack\u001b[1;34m(tup, dtype, casting)\u001b[0m\n\u001b[0;32m    357\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _nx\u001b[38;5;241m.\u001b[39mconcatenate(arrs, \u001b[38;5;241m0\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mdtype, casting\u001b[38;5;241m=\u001b[39mcasting)\n\u001b[0;32m    358\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 359\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_nx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcasting\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcasting\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mValueError\u001b[0m: all the input array dimensions except for the concatenation axis must match exactly, but along dimension 0, the array at index 0 has size 0 and the array at index 1 has size 50"
     ]
    }
   ],
   "source": [
    "identity = dpath.LMH.H002.value.st_center\n",
    "\n",
    "train = train_datasets(identity)\n",
    "test = test_datasets(identity)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 2570 is out of bounds for axis 0 with size 700",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 21\u001b[0m\n\u001b[0;32m     18\u001b[0m n_train, n_test \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# 学習\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_train\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# 入力と正解ラベルをGPU上に移動\u001b[39;49;00m\n",
      "File \u001b[1;32mc:\\Users\\satou\\anaconda3\\envs\\py311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\satou\\anaconda3\\envs\\py311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\satou\\anaconda3\\envs\\py311\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\satou\\anaconda3\\envs\\py311\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[4], line 32\u001b[0m, in \u001b[0;36mtrain_datasets.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mtensor, torch\u001b[38;5;241m.\u001b[39mtensor]:\n\u001b[1;32m---> 32\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_cepstrum\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_posture[idx]\n",
      "\u001b[1;31mIndexError\u001b[0m: index 2570 is out of bounds for axis 0 with size 700"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# モデルのインスタンス化\n",
    "net = Net(n_input, n_output, n_hidden).to(device)\n",
    "\n",
    "# 誤差関数を交差エントロピーで計算\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# 最適化アルゴリズム\n",
    "optimizer = optim.Adam(net.parameters(), lr = 0.1)\n",
    "\n",
    "# 学習\n",
    "n_epoch = 100\n",
    "for epoch in range(n_epoch):\n",
    "    # 精度と損失の初期化\n",
    "    train_acc, train_loss = 0, 0\n",
    "    val_acc, val_loss = 0, 0\n",
    "    n_train, n_test = 0, 0\n",
    "\n",
    "    # 学習\n",
    "    for input, label in train_loader:\n",
    "        n_train += len(label)\n",
    "\n",
    "        # 入力と正解ラベルをGPU上に移動\n",
    "        input = input.to(device)\n",
    "        label = label.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = net(input)\n",
    "        loss = criterion(output, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        predicted = torch.max(output, 1)[1]\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        train_acc += (predicted == label).sum().item()\n",
    "\n",
    "    # 検証\n",
    "    for test_input, test_label in test_loader:\n",
    "        n_test += 1\n",
    "\n",
    "        test_input = test_input.to(device)\n",
    "        test_label = test_label.to(device)\n",
    "\n",
    "        test_output = net(test_input)\n",
    "        test_loss = criterion(test_output, test_label)\n",
    "\n",
    "        test_predicted = torch.max(test_output, 1)[1]\n",
    "\n",
    "        test_loss += test_loss.item()\n",
    "        val_acc += (test_predicted == test_label).sum().item()\n",
    "\n",
    "    # 精度を確率に変換\n",
    "    train_acc /= n_train\n",
    "    val_acc /= n_test\n",
    "\n",
    "    print(f\"Epoch[{epoch+1}/{n_epoch}], | loss: {train_loss:.5f} | acc: {train_acc:.5f} | val_loss: {val_loss:.5f} | val_acc: {val_acc:.5f}\")\n",
    "    items = np.array([epoch+1, train_loss, train_acc, val_loss, val_acc])\n",
    "    history = np.vstack((history, items))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
